{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of control, low, medium and high progression rate using LSTM\n",
    "#### Predictions are made using bl, m06 and m12 dataset. Control, Low, Medium and High class is predicted at m48."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as torchUtils\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import random\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics \n",
    "import pandas as pd\n",
    "from sklearn import decomposition\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn import decomposition\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "mpl.style.use('seaborn-colorblind')\n",
    "import imageio\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already present\n",
      "Directory already present\n",
      "Directory already present\n",
      "Directory already present\n",
      "Directory already present\n",
      "Directory already present\n",
      "Directory already present\n",
      "Directory already present\n",
      "Directory already present\n",
      "Directory already present\n",
      "Directory already present\n",
      "Directory already present\n"
     ]
    }
   ],
   "source": [
    "#Keeping the directory correct\n",
    "import os\n",
    "os.chdir(\"C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\sem4\\\\\") # default directory\n",
    "\n",
    "# Create directoy to save processed data and results\n",
    "if not os.path.exists('adni_lstm_predict_label_data'):\n",
    "    os.makedirs('adni_lstm_predict_label_data')\n",
    "else:\n",
    "    print('Directory already present')\n",
    "    \n",
    "# Create directoy to save processed data and results\n",
    "if not os.path.exists('adni_lstm_predict_label_data\\\\progression_labels'):\n",
    "    os.makedirs('adni_lstm_predict_label_data\\\\progression_labels')\n",
    "else:\n",
    "    print('Directory already present')\n",
    "    \n",
    "for i in range(1,6):\n",
    "    if not os.path.exists('adni_lstm_predict_label_data\\\\progression_labels\\\\split_' + str(i) + '_m24'):\n",
    "        os.makedirs('adni_lstm_predict_label_data\\\\progression_labels\\\\split_' + str(i) + '_m24')\n",
    "    else:\n",
    "        print('Directory already present')\n",
    "\n",
    "        \n",
    "for i in range(1,6):\n",
    "    if not os.path.exists('adni_lstm_predict_label_data\\\\progression_labels\\\\split_' + str(i) + '_m48'):\n",
    "        os.makedirs('adni_lstm_predict_label_data\\\\progression_labels\\\\split_' + str(i) + '_m48')\n",
    "    else:\n",
    "        print('Directory already present')\n",
    "        \n",
    "os.chdir('C:\\\\Users\\\\Vipul Satone\\\\health data') # default directory\n",
    "address = \"C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\sem4\\\\\" # directory where images are to be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assessment data\n",
    "cols = {}\n",
    "list_months_to_be_considered = ['bl','m06','m12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = 'm48'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Following function gives information about dataset to be imputed.\n",
    "def data_info(dataset):\n",
    "    print('Name of dataset is: ' + dataset.name) \n",
    "    print('\\n0th level of columns is ')\n",
    "    print(list(pd.Series(dataset.columns.get_level_values(0)).unique()) )\n",
    "    print('\\n1st level of columns is: ')\n",
    "    print(list(pd.Series(dataset.columns.get_level_values(1)).unique()) )\n",
    "    print('\\nShape of datset is:')\n",
    "    print(dataset.shape)\n",
    "    print('\\nTotal number of missing values: ')\n",
    "    print(dataset.isnull().sum().sum())\n",
    "    \n",
    "    \n",
    "\n",
    "# Argument Train1 is dta to be normalized. IF argument b is 'z' the z normalization is done otherwise minmax normalization is done.\n",
    "def normalize(Train1,b):\n",
    "    col_names = list(Train1.columns)\n",
    "    Train1 = pd.DataFrame(Train1)\n",
    "    if (b == 'z'):\n",
    "        for i in range(Train1.shape[1]):\n",
    "            Train1[col_names[i]] = (Train1[col_names[i]] - Train1[col_names[i]].mean(skipna = True)) / Train1[col_names[i]].std(skipna = True)\n",
    "    else:\n",
    "        for i in range(Train1.shape[1]):\n",
    "            Train1[col_names[i]] = (Train1[col_names[i]] - min(Train1[col_names[i]]) )/ ( max(Train1[col_names[i]] ) - min(Train1[col_names[i]]) )\n",
    "    return Train1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10414, 7)\n"
     ]
    }
   ],
   "source": [
    "#CDR\n",
    "cols['cdr'] = ['RID','VISCODE2', 'CDMEMORY', 'CDORIENT', 'CDJUDGE' ,'CDCOMMUN' ,'CDHOME' ,'CDCARE']\n",
    "cdr = pd.read_csv(\"ADNI\\\\Raw_Data\\\\Assessment\\\\CDR.csv\",index_col='RID', usecols=cols['cdr'])\n",
    "cdr1 = cdr.copy(deep = True)\n",
    "print(cdr1.shape)\n",
    "cdr = cdr[cdr['VISCODE2'].isin(['bl','m12','m06']) ]  \n",
    "cdr = cdr.reset_index().set_index(['RID','VISCODE2'])\n",
    "cdr = cdr[~cdr.index.duplicated()].unstack()\n",
    "cdr = cdr[ (cdr.isnull().sum(axis = 1) <= 4) ]\n",
    "cdr = cdr.T\n",
    "cdr = cdr[cdr.index.get_level_values(1).isin(list_months_to_be_considered)].T\n",
    "# reducing index level\n",
    "cdr_ruf = cdr.T.reset_index()\n",
    "cdr_ruf.iloc[:,0]  = 'cdr__' + cdr_ruf.iloc[:,0] +  '___' + cdr_ruf.iloc[:,1]\n",
    "cdr_ruf = cdr_ruf.set_index('level_0')\n",
    "cdr = cdr_ruf.iloc[:,1:].T\n",
    "cdr_no_encoding = cdr\n",
    "\n",
    "cdr.name = 'Clinical Dementia Rating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12484, 48)\n"
     ]
    }
   ],
   "source": [
    "#NEUROBAT - Just using the total scores CLCOKSCOR, COPYSCOR, BNTTOTAL\n",
    "cols['neurobat'] = ['RID', 'VISCODE2', 'CLOCKSCOR', 'COPYSCOR', 'LMSTORY', 'LIMMTOTAL', 'LIMMEND',\n",
    "       'AVTOT1', 'AVERR1', 'AVTOT2', 'AVERR2', 'AVTOT3', 'AVERR3', 'AVTOT4',\n",
    "       'AVERR4', 'AVTOT5', 'AVERR5', 'AVTOT6', 'AVERR6', 'AVTOTB', 'AVERRB',\n",
    "       'AVENDED', 'DSPANFOR', 'DSPANFLTH', 'DSPANBAC', 'DSPANBLTH',\n",
    "       'CATANIMSC', 'CATANPERS', 'CATANINTR', 'CATVEGESC', 'CATVGPERS',\n",
    "       'CATVGINTR', 'TRAASCOR', 'TRAAERRCOM', 'TRAAERROM', 'TRABSCOR',\n",
    "       'TRABERRCOM', 'TRABERROM', 'DIGITSCOR', 'LDELBEGIN', 'LDELTOTAL',\n",
    "       'LDELCUE','BNTTOTAL', 'AVDELBEGAN', 'AVDEL30MIN', 'AVDELERR1',\n",
    "       'AVDELTOT', 'AVDELERR2', 'ANARTND', 'ANARTERR']\n",
    "neurobat_1 = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat'], index_col = ['RID', 'VISCODE2'])\n",
    "print(neurobat_1.shape)\n",
    "cols['neurobat_clock'] = ['RID', 'VISCODE2', 'CLOCKSCOR']\n",
    "neurobat_clock = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat_clock'], index_col = ['RID', 'VISCODE2'])\n",
    "neurobat_clock1 = neurobat_clock.copy(deep = True) \n",
    "neurobat_clock = neurobat_clock[~neurobat_clock.index.duplicated()].reset_index()\n",
    "neurobat_clock = neurobat_clock[neurobat_clock.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "neurobat_clock = neurobat_clock.unstack()\n",
    "neurobat_clock = neurobat_clock[ (neurobat_clock.isnull().sum(axis = 1) <= 1) ]\n",
    "new_col_list_neurobat_clock = neurobat_clock.columns.levels[0]\n",
    "for a in new_col_list_neurobat_clock: \n",
    "    neurobat_clock[a] = neurobat_clock[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "neurobat_clock.name = 'Neuropsychological Battery (subdata - clock)'\n",
    "#data_info(neurobat_clock)\n",
    "\n",
    "cols['neurobat_copy'] = ['RID', 'VISCODE2', 'COPYSCOR']\n",
    "neurobat_copy = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat_copy'], index_col = ['RID', 'VISCODE2'])\n",
    "neurobat_copy1 = neurobat_copy.copy(deep = True) \n",
    "neurobat_copy = neurobat_copy[~neurobat_copy.index.duplicated()].reset_index()\n",
    "neurobat_copy = neurobat_copy[neurobat_copy.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "neurobat_copy = neurobat_copy.unstack()\n",
    "neurobat_copy = neurobat_copy[ (neurobat_copy.isnull().sum(axis = 1) <= 1) ]\n",
    "new_col_list_neurobat_copy = neurobat_copy.columns.levels[0]\n",
    "for a in new_col_list_neurobat_copy: \n",
    "    neurobat_copy[a] = neurobat_copy[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "neurobat_copy.name = 'Neuropsychological Battery (subdata - copy)'\n",
    "#data_info(neurobat_copy)\n",
    "\n",
    "cols['neurobat_limm_story'] = ['RID', 'VISCODE2', 'LIMMTOTAL']\n",
    "neurobat_limm_story = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat_limm_story'], index_col = ['RID', 'VISCODE2'])\n",
    "neurobat_limm_story1 = neurobat_limm_story.copy(deep = True) \n",
    "neurobat_limm_story = neurobat_limm_story[~neurobat_limm_story.index.duplicated()].reset_index()\n",
    "neurobat_limm_story = neurobat_limm_story[neurobat_limm_story.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "#neurobat_clock = neurobat_clock[ (neurobat_clock.isnull().sum(axis = 1) <= 1) ]\n",
    "neurobat_limm_story = neurobat_limm_story.unstack()\n",
    "neurobat_limm_story = neurobat_limm_story.drop(['m06','bl'], axis=1, level=1)\n",
    "#neurobat_limm_story = neurobat_limm_story.T[ (neurobat_limm_story.columns.levels[1]) == 'm12' ].T\n",
    "neurobat_limm_story = neurobat_limm_story[ (neurobat_limm_story.isnull().sum(axis = 1) < 1) ]\n",
    "neurobat_limm_story.name = 'Neuropsychological Battery (subdata - story)'\n",
    "#data_info(neurobat_limm_story)\n",
    "\n",
    "cols['neurobat_dspan'] = ['RID', 'VISCODE2','DSPANFOR', 'DSPANFLTH', 'DSPANBAC', 'DSPANBLTH']\n",
    "neurobat_dspan = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat_dspan'], index_col = ['RID', 'VISCODE2'])\n",
    "neurobat_dspan1 = neurobat_dspan.copy(deep = True) \n",
    "neurobat_dspan = neurobat_dspan[~neurobat_dspan.index.duplicated()].reset_index()\n",
    "neurobat_dspan = neurobat_dspan[neurobat_dspan.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "neurobat_dspan = neurobat_dspan[ (neurobat_dspan.isnull().sum(axis = 1) < 4) ]\n",
    "neurobat_dspan = neurobat_dspan.unstack()\n",
    "neurobat_dspan = neurobat_dspan[ (neurobat_dspan.isnull().sum(axis = 1) <6) ]\n",
    "new_col_list_neurobat_dspan = neurobat_dspan.columns.levels[0]\n",
    "for a in new_col_list_neurobat_dspan: \n",
    "    neurobat_dspan[a] = neurobat_dspan[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "neurobat_dspan.name = 'Neuropsychological Battery (subdata - digit span)'\n",
    "#data_info(neurobat_dspan)\n",
    "\n",
    "cols['neurobat_cat_flu'] = ['RID', 'VISCODE2','CATANIMSC', 'CATANPERS', 'CATANINTR', 'CATVEGESC', 'CATVGPERS','CATVGINTR']\n",
    "neurobat_cat_flu = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat_cat_flu'], index_col = ['RID', 'VISCODE2'])\n",
    "neurobat_cat_flu1 = neurobat_cat_flu.copy(deep = True) \n",
    "neurobat_cat_flu = neurobat_cat_flu[~neurobat_cat_flu.index.duplicated()].reset_index()\n",
    "neurobat_cat_flu = neurobat_cat_flu[neurobat_cat_flu.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "neurobat_cat_flu = neurobat_cat_flu.replace({-1: np.NAN})\n",
    "neurobat_cat_flu = neurobat_cat_flu[ (neurobat_cat_flu.isnull().sum(axis = 1) < 4) ]\n",
    "del neurobat_cat_flu['CATVEGESC']\n",
    "del neurobat_cat_flu['CATVGPERS']\n",
    "del neurobat_cat_flu['CATVGINTR']\n",
    "neurobat_cat_flu = neurobat_cat_flu.unstack()\n",
    "neurobat_cat_flu = neurobat_cat_flu[ (neurobat_cat_flu.isnull().sum(axis = 1) <4) ]\n",
    "new_col_list_neurobat_cat_flu = neurobat_cat_flu.columns.levels[0]\n",
    "for a in new_col_list_neurobat_cat_flu: \n",
    "    neurobat_cat_flu[a] = neurobat_cat_flu[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "neurobat_cat_flu.name = 'Neuropsychological Battery (subdata - category fluency : only animal examples)'\n",
    "#data_info(neurobat_cat_flu)\n",
    "\n",
    "cols['neurobat_trail'] = ['RID', 'VISCODE2', 'TRAASCOR', 'TRAAERRCOM', 'TRAAERROM', 'TRABSCOR','TRABERRCOM', 'TRABERROM']\n",
    "neurobat_trail = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat_trail'], index_col = ['RID', 'VISCODE2'])\n",
    "neurobat_trail1 = neurobat_trail.copy(deep = True) \n",
    "neurobat_trail = neurobat_trail[~neurobat_trail.index.duplicated()].reset_index()\n",
    "neurobat_trail = neurobat_trail[neurobat_trail.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "neurobat_trail = neurobat_trail[ (neurobat_trail.isnull().sum(axis = 1) < 3) ]\n",
    "neurobat_trail = neurobat_trail.unstack()\n",
    "neurobat_trail = neurobat_trail[ (neurobat_trail.isnull().sum(axis = 1) <=6) ]\n",
    "new_col_list_neurobat_trail = neurobat_trail.columns.levels[0]\n",
    "for a in new_col_list_neurobat_trail: \n",
    "    neurobat_trail[a] = neurobat_trail[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "neurobat_trail.name = 'Neuropsychological Battery (subdata - Trail making)'\n",
    "#data_info(neurobat_trail)\n",
    "\n",
    "cols['neurobat_av'] = ['RID', 'VISCODE2','AVTOT1', 'AVDELERR1','AVDELTOT', 'AVERR1', 'AVTOT2', 'AVERR2', 'AVTOT3',     'AVERR3','AVDELERR2', 'AVTOT4','AVERR4', 'AVTOT5', 'AVERR5', 'AVTOT6', 'AVERR6', 'AVTOTB', 'AVERRB','AVDEL30MIN']\n",
    "neurobat_av = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat_av'], index_col = ['RID', 'VISCODE2'])\n",
    "neurobat_av1 = neurobat_av.copy(deep = True) \n",
    "neurobat_av = neurobat_av[~neurobat_av.index.duplicated()].reset_index()\n",
    "neurobat_av = neurobat_av[neurobat_av.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "neurobat_av = neurobat_av.unstack()\n",
    "neurobat_av = neurobat_av[ (neurobat_av.isnull().sum(axis = 1) <25) ]\n",
    "new_col_list_neurobat_av = neurobat_av.columns.levels[0]\n",
    "for a in new_col_list_neurobat_av: \n",
    "    neurobat_av[a] = neurobat_av[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "neurobat_av.name = 'Neuropsychological Battery (subdata - av)'\n",
    "#data_info(neurobat_av)\n",
    "\n",
    "cols['neurobat_digit_score'] = ['RID', 'VISCODE2','DIGITSCOR']\n",
    "neurobat_digit_score = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat_digit_score'], index_col = ['RID', 'VISCODE2'])\n",
    "neurobat_digit_score1 = neurobat_digit_score.copy(deep = True) \n",
    "neurobat_digit_score = neurobat_digit_score[~neurobat_digit_score.index.duplicated()].reset_index()\n",
    "neurobat_digit_score = neurobat_digit_score[neurobat_digit_score.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "neurobat_digit_score = neurobat_digit_score[ (neurobat_digit_score.isnull().sum(axis = 1) < 1) ]\n",
    "#neurobat_clock = neurobat_clock[ (neurobat_clock.isnull().sum(axis = 1) <= 1) ]\n",
    "neurobat_digit_score = neurobat_digit_score.unstack()\n",
    "neurobat_digit_score = neurobat_digit_score[ (neurobat_digit_score.isnull().sum(axis = 1) <=1) ]\n",
    "new_col_list_neurobat_digit_score = neurobat_digit_score.columns.levels[0]\n",
    "for a in new_col_list_neurobat_digit_score: \n",
    "    neurobat_digit_score[a] = neurobat_digit_score[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "neurobat_digit_score.name = 'Neuropsychological Battery (subdata - digit score)'\n",
    "#data_info(neurobat_digit_score)\n",
    "\n",
    "cols['neurobat_logical_memory'] = ['RID', 'VISCODE2','LDELTOTAL']\n",
    "neurobat_logical_memory = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat_logical_memory'], index_col = ['RID', 'VISCODE2'])\n",
    "neurobat_logical_memory1 = neurobat_logical_memory.copy(deep = True) \n",
    "neurobat_logical_memory = neurobat_logical_memory[~neurobat_logical_memory.index.duplicated()].reset_index()\n",
    "neurobat_logical_memory = neurobat_logical_memory[neurobat_logical_memory.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "neurobat_logical_memory = neurobat_logical_memory[ (neurobat_logical_memory.isnull().sum(axis = 1) < 1) ]\n",
    "neurobat_logical_memory = neurobat_logical_memory.unstack()\n",
    "neurobat_logical_memory.name = 'Neuropsychological Battery (subdata - logical memeory test)'\n",
    "#data_info(neurobat_logical_memory)\n",
    "\n",
    "cols['neurobat_boston_naming_test'] = ['RID', 'VISCODE2', 'BNTSPONT','BNTSTIM','BNTCSTIM','BNTPHON','BNTCPHON']\n",
    "neurobat_boston_naming_test = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat_boston_naming_test'], index_col = ['RID', 'VISCODE2'])\n",
    "neurobat_boston_naming_test1 = neurobat_boston_naming_test.copy(deep = True) \n",
    "neurobat_boston_naming_test = neurobat_boston_naming_test[~neurobat_boston_naming_test.index.duplicated()].reset_index()\n",
    "neurobat_boston_naming_test = neurobat_boston_naming_test[neurobat_boston_naming_test.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "neurobat_boston_naming_test = neurobat_boston_naming_test[ (neurobat_boston_naming_test.isnull().sum(axis = 1) < 5) ]\n",
    "neurobat_boston_naming_test = neurobat_boston_naming_test.unstack()\n",
    "neurobat_boston_naming_test = neurobat_boston_naming_test[ (neurobat_boston_naming_test.isnull().sum(axis = 1) <6) ]\n",
    "new_col_list_neurobat_boston_naming_test = neurobat_boston_naming_test.columns.levels[0]\n",
    "for a in new_col_list_neurobat_boston_naming_test: \n",
    "    neurobat_boston_naming_test[a] = neurobat_boston_naming_test[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "neurobat_boston_naming_test.name = 'Neuropsychological Battery (subdata - Boston naming test)'\n",
    "#data_info(neurobat_boston_naming_test)\n",
    "\n",
    "cols['neurobat_anrt'] = ['RID', 'VISCODE2', 'ANARTND']\n",
    "neurobat_anrt = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat_anrt'], index_col = ['RID', 'VISCODE2'])\n",
    "neurobat_anrt1 = neurobat_anrt.copy(deep = True) \n",
    "neurobat_anrt = neurobat_anrt[~neurobat_anrt.index.duplicated()].reset_index()\n",
    "neurobat_anrt = neurobat_anrt[neurobat_anrt.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "neurobat_anrt = neurobat_anrt[ (neurobat_anrt.isnull().sum(axis = 1) < 1) ]\n",
    "#neurobat_clock = neurobat_clock[ (neurobat_clock.isnull().sum(axis = 1) <= 1) ]\n",
    "neurobat_anrt = neurobat_anrt.unstack()\n",
    "neurobat_anrt = neurobat_anrt[ (neurobat_anrt.isnull().sum(axis = 1) <=1) ]\n",
    "new_col_list_neurobat_anrt = neurobat_anrt.columns.levels[0]\n",
    "for a in new_col_list_neurobat_anrt: \n",
    "    neurobat_anrt[a] = neurobat_anrt[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "neurobat_anrt.name = 'Neuropsychological Battery (subdata - American national reading test)'\n",
    "#data_info(neurobat_anrt)\n",
    "\n",
    "neurobat1 = pd.merge(neurobat_clock,neurobat_copy , left_index = True, right_index = True, how='inner')\n",
    "neurobat1 = pd.merge(neurobat1,neurobat_limm_story , left_index = True, right_index = True, how='inner')\n",
    "neurobat1 = pd.merge(neurobat1,neurobat_av , left_index = True, right_index = True, how='inner')\n",
    "neurobat1 = pd.merge(neurobat1,neurobat_cat_flu , left_index = True, right_index = True, how='inner')\n",
    "neurobat1 = pd.merge(neurobat1,neurobat_trail , left_index = True, right_index = True, how='inner')\n",
    "neurobat1 = pd.merge(neurobat1,neurobat_logical_memory , left_index = True, right_index = True, how='inner')\n",
    "neurobat1 = pd.merge(neurobat1,neurobat_boston_naming_test , left_index = True, right_index = True, how='inner')\n",
    "neurobat1 = neurobat1.T\n",
    "neurobat1 = neurobat1[neurobat1.index.get_level_values(1).isin(list_months_to_be_considered)].T\n",
    "# reducing index level\n",
    "neurobat1_ruf = neurobat1.T.reset_index()\n",
    "neurobat1_ruf.iloc[:,0]  = 'neurobat__' + neurobat1_ruf.iloc[:,0] +  '___' + neurobat1_ruf.iloc[:,1]\n",
    "neurobat1_ruf = neurobat1_ruf.set_index('level_0')\n",
    "neurobat1 = neurobat1_ruf.iloc[:,1:].T\n",
    "neurobat1_no_encoding = neurobat1\n",
    "\n",
    "neurobat = neurobat1\n",
    "\n",
    "neurobat.name = 'Neuropsychological Battery (All combined)'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10617, 1)\n"
     ]
    }
   ],
   "source": [
    "#MMSE\n",
    "#cols['mmse'] = ['RID', 'VISCODE2','MMSCORE','MMDATE','MMYEAR','MMMONTH','MMDAY','MMSEASON','MMHOSPIT',    'MMFLOOR','MMCITY','MMAREA','MMSTATE','MMBALL','MMFLAG','MMTREE','MMD','MML','MMR','MMO','MMW',    'MMBALLDL','MMFLAGDL','MMTREEDL','MMWATCH','MMPENCIL','MMREPEAT','MMHAND','MMFOLD','MMONFLR','MMREAD',    'MMWRITE','MMDRAW']\n",
    "cols['mmse'] = ['RID', 'VISCODE2','MMSCORE']\n",
    "\n",
    "mmse = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\MMSE.csv', usecols=cols['mmse'], index_col = ['RID', 'VISCODE2'])\n",
    "mmse1 = mmse.copy(deep = True)\n",
    "print(mmse1.shape)\n",
    "mmse = mmse[~mmse.index.duplicated()].reset_index()\n",
    "mmse = mmse[mmse.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "mmse = mmse.replace({-4:np.NAN})\n",
    "mmse = mmse.replace({-1:np.NAN})\n",
    "mmse = mmse[ (mmse.isnull().sum(axis = 1) < 10) ]\n",
    "mmse = mmse.unstack()\n",
    "mmse = mmse[ (mmse.isnull().sum(axis = 1) < 20) ]\n",
    "mmse = mmse[~mmse.index.duplicated()]\n",
    "mmse = mmse.T\n",
    "mmse = mmse[mmse.index.get_level_values(1).isin(list_months_to_be_considered)].T\n",
    "\n",
    "# reducing index level\n",
    "mmse_ruf = mmse.T.reset_index()\n",
    "mmse_ruf.iloc[:,0]  = 'mmse__' + mmse_ruf.iloc[:,0] +  '___' + mmse_ruf.iloc[:,1]\n",
    "mmse_ruf = mmse_ruf.set_index('level_0')\n",
    "mmse = mmse_ruf.iloc[:,1:].T\n",
    "\n",
    "\n",
    "mmse_no_encoding = mmse\n",
    "## Hot encoding\n",
    "#mmse_name_list = list( mmse.columns )\n",
    "#mmse_empty = pd.DataFrame()\n",
    "#for i in range(len(mmse_name_list)):\n",
    "#    name = mmse_name_list[i]\n",
    "#    mmse_with_dummies = pd.get_dummies(mmse[name], sparse=True, drop_first=True, prefix=name)\n",
    "#    mmse_empty = pd.concat([mmse_empty,mmse_with_dummies] , axis = 1)   \n",
    "mmse = mmse_no_encoding    \n",
    "mmse.name = 'Mini Mental State Exam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9486, 2)\n"
     ]
    }
   ],
   "source": [
    "#GERIATRIC\n",
    "cols['geriatric'] = ['VISCODE2', 'RID', 'GDTOTAL']\n",
    "geriatric = pd.read_csv(\"ADNI\\\\Raw_Data\\\\Assessment\\\\GDSCALE.csv\", index_col='RID', usecols=cols['geriatric'])\n",
    "geriatric1 = geriatric.copy(deep = True)\n",
    "print(geriatric1.shape)\n",
    "geriatric = geriatric.replace({-4:np.NAN})\n",
    "geriatric = geriatric.replace({-1:np.NAN})\n",
    "geriatric = geriatric[geriatric['VISCODE2'].isin(['bl','m12','m06']) ]  \n",
    "geriatric = geriatric.reset_index().set_index(['RID','VISCODE2'])\n",
    "geriatric = geriatric[ (geriatric.isnull().sum(axis = 1) ==0) ]\n",
    "geriatric = geriatric[~geriatric.index.duplicated()].unstack()\n",
    "geriatric = geriatric[ (geriatric.isnull().sum(axis = 1) ==0) ]\n",
    "geriatric = geriatric.T\n",
    "geriatric = geriatric[geriatric.index.get_level_values(1).isin(list_months_to_be_considered)].T\n",
    "\n",
    "\n",
    "# reducing index level\n",
    "geriatric_ruf = geriatric.T.reset_index()\n",
    "geriatric_ruf.iloc[:,0]  = 'gd_scale__' + geriatric_ruf.iloc[:,0] +  '___' + geriatric_ruf.iloc[:,1]\n",
    "geriatric_ruf = geriatric_ruf.set_index('level_0')\n",
    "geriatric = geriatric_ruf.iloc[:,1:].T\n",
    "geriatric_no_encoding = geriatric\n",
    "\n",
    "geriatric.name = 'Geriatric depression scale'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9207, 2)\n"
     ]
    }
   ],
   "source": [
    "#UWNPSYCHSUM_10_27_17\n",
    "cols['UWNPSYCHSUM_10_27_17'] = ['RID', 'VISCODE2', 'ADNI_MEM', 'ADNI_EF']\n",
    "uwn = pd.DataFrame(pd.read_csv(\"ADNI\\\\Raw_Data\\\\Assessment\\\\UWNPSYCHSUM_10_27_17.csv\",index_col= ['RID','VISCODE2'], usecols=cols['UWNPSYCHSUM_10_27_17']))\n",
    "uwn1 = uwn.copy(deep = True)\n",
    "print(uwn1.shape)\n",
    "uwn = uwn[~uwn.index.duplicated()].reset_index()\n",
    "uwn = uwn[uwn.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "uwn = uwn[~uwn.index.duplicated()].unstack()\n",
    "uwn = uwn[ (uwn.isnull().sum(axis = 1) < 3) ]\n",
    "new_col_list_uwn = uwn.columns.levels[0]\n",
    "for a in new_col_list_uwn: \n",
    "    uwn[a] = uwn[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "uwn = uwn[ (uwn.isnull().sum(axis = 1) <1) ]\n",
    "uwn = uwn.T\n",
    "uwn = uwn[uwn.index.get_level_values(1).isin(list_months_to_be_considered)].T\n",
    "\n",
    "\n",
    "\n",
    "# reducing index level\n",
    "uwn_ruf = uwn.T.reset_index()\n",
    "uwn_ruf.iloc[:,0]  = 'UW__' + uwn_ruf.iloc[:,0] +  '___' + uwn_ruf.iloc[:,1]\n",
    "uwn_ruf = uwn_ruf.set_index('level_0')\n",
    "uwn = uwn_ruf.iloc[:,1:].T\n",
    "uwn_no_encoding = uwn\n",
    "uwn.name = 'Crane Lab (UW) Neuropsych Summary Score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4447, 12)\n"
     ]
    }
   ],
   "source": [
    "#NPI\n",
    "cols['npi'] = ['RID', 'VISCODE2','NPIATOT', 'NPIBTOT', 'NPICTOT',  'NPIDTOT',     'NPIETOT', 'NPIFTOT', 'NPIGTOT', 'NPIHTOT', 'NPIITOT', 'NPIJTOT','NPIKTOT', 'NPILTOT']\n",
    "npi = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NPI.csv', usecols=cols['npi'], index_col = ['RID', 'VISCODE2'])\n",
    "npi1 = npi.copy(deep = True)\n",
    "print(npi1.shape)\n",
    "npi = npi[~npi.index.duplicated()].reset_index()\n",
    "# m12 only\n",
    "npi_m12 = npi[npi.VISCODE2.isin(['m12'])].set_index(['RID','VISCODE2'])\n",
    "npi_m12 = npi_m12[~npi_m12.index.duplicated()].unstack()\n",
    "npi_m12 = npi_m12[ (npi_m12.isnull().sum(axis = 1) < 12) ]\n",
    "# baseline\n",
    "npi_bl = npi[npi.VISCODE2.isin(['bl'])].set_index(['RID','VISCODE2'])\n",
    "npi_bl = npi_bl[~npi_bl.index.duplicated()].unstack()\n",
    "npi_bl = npi_bl[ (npi_bl.isnull().sum(axis = 1) < 12) ]\n",
    "# both baseline and m12\n",
    "npi_all = npi[npi.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "npi_all = npi_all[~npi_all.index.duplicated()].unstack()\n",
    "npi_all = npi_all[ (npi_all.isnull().sum(axis = 1) < 10) ]\n",
    "npi_all = npi_all.T\n",
    "npi_all = npi_all[npi_all.index.get_level_values(1).isin(list_months_to_be_considered)].T\n",
    "\n",
    "# reducing index level\n",
    "npi_all_ruf = npi_all.T.reset_index()\n",
    "npi_all_ruf.iloc[:,0]  = 'npi_all__' + npi_all_ruf.iloc[:,0] +  '___' + npi_all_ruf.iloc[:,1]\n",
    "npi_all_ruf = npi_all_ruf.set_index('level_0')\n",
    "npi_all = npi_all_ruf.iloc[:,1:].T\n",
    "npi_all_no_encoding = npi_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5509, 10)\n"
     ]
    }
   ],
   "source": [
    "#MOCA \n",
    "cols['moca'] = ['RID','VISCODE2', 'TRAILS', 'CUBE', 'CLOCKCON', 'CLOCKNO', 'CLOCKHAN','LION', 'RHINO', 'CAMEL', 'IMMT1W1', 'IMMT1W2', 'IMMT1W3', 'IMMT1W4',\n",
    "       'IMMT1W5', 'IMMT2W1', 'IMMT2W2', 'IMMT2W3', 'IMMT2W4', 'IMMT2W5','DIGFOR', 'DIGBACK', 'LETTERS', 'SERIAL1', 'SERIAL2', 'SERIAL3',\n",
    "       'SERIAL4', 'SERIAL5', 'REPEAT1', 'REPEAT2', 'FFLUENCY', 'ABSTRAN','ABSMEAS', 'DELW1', 'DELW2', 'DELW3', 'DELW4', 'DELW5', 'DATE', 'MONTH',\n",
    "       'YEAR', 'DAY', 'PLACE', 'CITY']\n",
    "\n",
    "cols['moca_trail_making'] = ['TRAILS']\n",
    "cols['moca_visuosoconstructional'] = ['CUBE', 'CLOCKCON', 'CLOCKNO', 'CLOCKHAN']\n",
    "cols['moca_naming'] = [ 'LION', 'RHINO', 'CAMEL']\n",
    "cols['moca_immediate_recall'] = [ 'IMMT2W1', 'IMMT2W2', 'IMMT2W3', 'IMMT2W4', 'IMMT2W5','IMMT1W1', 'IMMT1W2', 'IMMT1W3', 'IMMT1W4', 'IMMT1W5']\n",
    "cols['moca_attention'] = [ 'DIGFOR', 'DIGBACK', 'LETTERS', 'SERIAL1', 'SERIAL2', 'SERIAL3','SERIAL4', 'SERIAL5']\n",
    "cols['moca_sen_repetetion'] = ['REPEAT1','REPEAT2']\n",
    "cols['moca_fluency'] = ['FFLUENCY']\n",
    "cols['moca_abstraction'] = ['ABSTRAN','ABSMEAS']\n",
    "cols['moca_delayed_word_recall'] = [ 'DELW1', 'DELW2', 'DELW3', 'DELW4', 'DELW5']\n",
    "cols['moca_orientation'] = [ 'DATE', 'MONTH', 'YEAR', 'DAY', 'PLACE', 'CITY' ]\n",
    "moca = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\MOCA.csv', usecols=cols['moca'], index_col = ['RID', 'VISCODE2'])\n",
    "moca['moca_trail_making'] = moca[cols['moca_trail_making']].sum(axis=1)\n",
    "moca['moca_visuosoconstructional'] = moca[cols['moca_visuosoconstructional']].sum(axis=1)\n",
    "moca['moca_naming'] = moca[cols['moca_naming']].sum(axis = 1)\n",
    "moca['moca_immediate_recall'] = moca[cols['moca_immediate_recall']].sum(axis=1)\n",
    "moca['moca_attention'] = moca[cols['moca_attention']].sum(axis=1)\n",
    "moca['moca_sen_repetetion'] = moca[cols['moca_sen_repetetion']].sum(axis=1)\n",
    "moca['moca_fluency'] = moca[cols['moca_fluency']].sum(axis=1)\n",
    "moca['moca_abstraction'] = moca[cols['moca_abstraction']].sum(axis=1)\n",
    "moca['moca_delayed_word_recall'] = moca[cols['moca_delayed_word_recall']].sum(axis=1)\n",
    "moca['moca_orientation'] = moca[cols['moca_orientation']].sum(axis=1)\n",
    "moca = moca[['moca_trail_making', 'moca_visuosoconstructional', 'moca_naming', 'moca_attention', 'moca_immediate_recall',             'moca_sen_repetetion', 'moca_fluency','moca_abstraction','moca_delayed_word_recall','moca_orientation']] # drop extra\n",
    "moca1 = moca.copy(deep = True) \n",
    "print(moca1.shape)\n",
    "#Dropping the Duplicated Index (Only 1)\n",
    "moca = moca[~moca.index.duplicated()].reset_index()\n",
    "moca = moca[moca.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "moca = moca.unstack()\n",
    "moca = moca[ (moca.isnull().sum(axis = 1) < 15) ]\n",
    "new_col_list_moca = moca.columns.levels[0]\n",
    "for a in new_col_list_moca: \n",
    "    moca[a] = moca[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "moca = moca.T\n",
    "moca = moca[moca.index.get_level_values(1).isin(list_months_to_be_considered)].T\n",
    "  \n",
    "    \n",
    "# reducing index level\n",
    "moca_ruf = moca.T.reset_index()\n",
    "moca_ruf.iloc[:,0]  = 'moca__' + moca_ruf.iloc[:,0] +  '___' + moca_ruf.iloc[:,1]\n",
    "moca_ruf = moca_ruf.set_index('level_0')\n",
    "moca = moca_ruf.iloc[:,1:].T\n",
    "moca_no_encoding = moca\n",
    "\n",
    "\n",
    "## Hot encoding\n",
    "#moca_name_list = list( moca.columns )\n",
    "#moca_empty = pd.DataFrame()\n",
    "#for i in range(len(moca_name_list)):\n",
    "#    name = moca_name_list[i]\n",
    "#    moca_with_dummies = pd.get_dummies(moca[name], sparse=True, drop_first=True, prefix=name)\n",
    "#    indexex = moca_with_dummies.index \n",
    "#    moca_empty = pd.concat([moca_empty,moca_with_dummies] , axis = 1)  \n",
    "#moca = moca_empty        \n",
    "\n",
    "moca = moca_no_encoding\n",
    "moca.name = 'Montreal Cognitive Assessment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9525, 1)\n"
     ]
    }
   ],
   "source": [
    "#FAQ\n",
    "cols['faq'] = ['RID', 'VISCODE2', 'FAQTOTAL']\n",
    "faq = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\FAQ.csv', usecols=cols['faq'], index_col = ['RID', 'VISCODE2'])\n",
    "faq1 = faq.copy(deep = True) \n",
    "print(faq1.shape)\n",
    "faq = faq[~faq.index.duplicated()].reset_index()\n",
    "faq = faq[faq.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "faq = faq[~faq.index.duplicated()]\n",
    "#Unstacking \n",
    "faq = faq.unstack()\n",
    "faq = faq[ (faq.isnull().sum(axis = 1) < 2) ]\n",
    "new_col_list_faq = faq.columns.levels[0]\n",
    "for a in new_col_list_faq: \n",
    "    faq[a] = faq[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "    \n",
    "faq = faq.T\n",
    "faq = faq[faq.index.get_level_values(1).isin(list_months_to_be_considered)].T\n",
    "      \n",
    "# reducing index level\n",
    "faq_ruf = faq.T.reset_index()\n",
    "faq_ruf.iloc[:,0]  = 'FAQ__' + faq_ruf.iloc[:,0] +  '___' + faq_ruf.iloc[:,1]\n",
    "faq_ruf = faq_ruf.set_index('level_0')\n",
    "faq = faq_ruf.iloc[:,1:].T\n",
    "faq_no_encoding = faq\n",
    "\n",
    "## Hot encoding\n",
    "#faq_name_list = list( faq.columns )\n",
    "#faq_empty = pd.DataFrame()\n",
    "#for i in range(len(faq_name_list)):\n",
    "#    name = faq_name_list[i]\n",
    "#    faq_with_dummies = pd.get_dummies(faq[name], sparse=True, drop_first=True, prefix=name)\n",
    "#    faq_empty = pd.concat([faq_empty,faq_with_dummies] , axis = 1)\n",
    "#faq = faq_empty    \n",
    "faq = faq_no_encoding\n",
    "faq.name = 'Functional Assessment Questionnaire'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5238, 40)\n"
     ]
    }
   ],
   "source": [
    "#ECOGPT\n",
    "cols['ecogpt'] =  ['RID', 'VISCODE2',        'MEMORY1', 'MEMORY2', 'MEMORY3', 'MEMORY4', 'MEMORY5',       'MEMORY6', 'MEMORY7', 'MEMORY8', 'LANG1', 'LANG2', 'LANG3', 'LANG4',       'LANG5', 'LANG6', 'LANG7', 'LANG8', 'LANG9', 'VISSPAT1', 'VISSPAT2',       'VISSPAT3', 'VISSPAT4', 'VISSPAT6', 'VISSPAT7', 'VISSPAT8',       'PLAN1', 'PLAN2', 'PLAN3', 'PLAN4', 'PLAN5', 'ORGAN1', 'ORGAN2',       'ORGAN3', 'ORGAN4', 'ORGAN5', 'ORGAN6', 'DIVATT1', 'DIVATT2', 'DIVATT3',       'DIVATT4']\n",
    "ecogpt = pd.read_csv(\"ADNI\\\\Raw_Data\\\\Assessment\\\\ecogpt.csv\",index_col='RID', usecols=cols['ecogpt'])\n",
    "\n",
    "list_memory = [ 'MEMORY1', 'MEMORY2', 'MEMORY3', 'MEMORY4', 'MEMORY5',       'MEMORY6', 'MEMORY7', 'MEMORY8']\n",
    "list_lang = ['LANG1', 'LANG2', 'LANG3', 'LANG4',       'LANG5', 'LANG6', 'LANG7', 'LANG8', 'LANG9']\n",
    "list_vis = ['VISSPAT1', 'VISSPAT2',       'VISSPAT3', 'VISSPAT4', 'VISSPAT6', 'VISSPAT7', 'VISSPAT8']\n",
    "list_plan = ['PLAN1', 'PLAN2', 'PLAN3', 'PLAN4', 'PLAN5']\n",
    "list_org = ['ORGAN1', 'ORGAN2',       'ORGAN3', 'ORGAN4', 'ORGAN5', 'ORGAN6']\n",
    "list_div = ['DIVATT1', 'DIVATT2', 'DIVATT3',       'DIVATT4']\n",
    "\n",
    "ecogpt_new = pd.DataFrame()\n",
    "ecogpt1 = ecogpt.copy(deep = True)\n",
    "print(ecogpt1.shape)\n",
    "ecogpt = ecogpt[ecogpt['VISCODE2'].isin(['bl','m12','m06']) ]  \n",
    "ecogpt = ecogpt.reset_index().set_index(['RID','VISCODE2'])\n",
    "ecogpt = ecogpt[~ecogpt.index.duplicated()]\n",
    "ecogpt = ecogpt[ (ecogpt.isnull().sum(axis = 1) <= 30) ]\n",
    "ecogpt = ecogpt.unstack()\n",
    "ecogpt = ecogpt[ (ecogpt.isnull().sum(axis = 1) < 41) ]\n",
    "new_col_list_ecogpt = ecogpt.columns.levels[0]\n",
    "for a in new_col_list_ecogpt: \n",
    "    ecogpt[a] = ecogpt[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "    \n",
    "ecogpt = ecogpt.replace({9: None })\n",
    "ecogpt = ecogpt.T\n",
    "ecogpt_bl = ecogpt[ecogpt.index.get_level_values(1).isin(['bl'])]\n",
    "ecogpt_m06 = ecogpt[ecogpt.index.get_level_values(1).isin(['m06'])]\n",
    "ecogpt_m12 = ecogpt[ecogpt.index.get_level_values(1).isin(['m12'])]\n",
    " \n",
    "ecogpt_new['memory___bl']  =  ecogpt_bl[ecogpt_bl.index.get_level_values(0).isin(list_memory)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['lang___bl']  =  ecogpt_bl[ecogpt_bl.index.get_level_values(0).isin(list_lang)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['vis___bl']  =  ecogpt_bl[ecogpt_bl.index.get_level_values(0).isin(list_vis)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['plan___bl']  =  ecogpt_bl[ecogpt_bl.index.get_level_values(0).isin(list_plan)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['org___bl']  =  ecogpt_bl[ecogpt_bl.index.get_level_values(0).isin(list_org)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['division___bl']  =  ecogpt_bl[ecogpt_bl.index.get_level_values(0).isin(list_div)].mean( axis = 0, skipna = True  )\n",
    "\n",
    "ecogpt_new['memory___m06']  =  ecogpt_m06[ecogpt_m06.index.get_level_values(0).isin(list_memory)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['lang___m06']  =  ecogpt_m06[ecogpt_m06.index.get_level_values(0).isin(list_lang)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['vis___m06']  =  ecogpt_m06[ecogpt_m06.index.get_level_values(0).isin(list_vis)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['plan___m06']  =  ecogpt_m06[ecogpt_m06.index.get_level_values(0).isin(list_plan)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['org___m06']  =  ecogpt_m06[ecogpt_m06.index.get_level_values(0).isin(list_org)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['division___m06']  =  ecogpt_m06[ecogpt_m06.index.get_level_values(0).isin(list_div)].mean( axis = 0, skipna = True  )\n",
    "\n",
    "ecogpt_new['memory___m12']  =  ecogpt_m12[ecogpt_m12.index.get_level_values(0).isin(list_memory)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['lang___m12']  =  ecogpt_m12[ecogpt_m12.index.get_level_values(0).isin(list_lang)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['vis___m12']  =  ecogpt_m12[ecogpt_m12.index.get_level_values(0).isin(list_vis)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['plan___m12']  =  ecogpt_m12[ecogpt_m12.index.get_level_values(0).isin(list_plan)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['org___m12']  =  ecogpt_m12[ecogpt_m12.index.get_level_values(0).isin(list_org)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['division___m12']  =  ecogpt_m12[ecogpt_m12.index.get_level_values(0).isin(list_div)].mean( axis = 0, skipna = True  )\n",
    "\n",
    "\n",
    "\n",
    "# reducing index level\n",
    "ecogpt_ruf = ecogpt_new.T.reset_index()\n",
    "ecogpt_ruf.iloc[:,0]  = 'ecogpt_'  + ecogpt_ruf.iloc[:,0]\n",
    "ecogpt_ruf = ecogpt_ruf.set_index(['index'])\n",
    "ecogpt_ruf = ecogpt_ruf.T\n",
    "ecogpt_no_encoding = ecogpt_ruf\n",
    "\n",
    "\n",
    "## Hot encoding\n",
    "#ecogpt_name_list = list( ecogpt.columns )\n",
    "#ecogpt_empty = pd.DataFrame()\n",
    "#for i in range(len(ecogpt_name_list)):\n",
    "#    name = ecogpt_name_list[i]\n",
    "#    ecogpt_with_dummies = pd.get_dummies(ecogpt[name], sparse=True, drop_first=True, prefix=name)\n",
    "#    ecogpt_empty = pd.concat([ecogpt_empty,ecogpt_with_dummies] , axis = 1)\n",
    "#    \n",
    "ecogpt = ecogpt_no_encoding   \n",
    "ecogpt.name = 'Everyday cognition - study partner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5463, 40)\n"
     ]
    }
   ],
   "source": [
    "#EXOGSP\n",
    "cols['ecogsp'] =  ['RID', 'VISCODE2',        'MEMORY1', 'MEMORY2', 'MEMORY3', 'MEMORY4', 'MEMORY5',       'MEMORY6', 'MEMORY7', 'MEMORY8', 'LANG1', 'LANG2', 'LANG3', 'LANG4',       'LANG5', 'LANG6', 'LANG7', 'LANG8', 'LANG9', 'VISSPAT1', 'VISSPAT2',       'VISSPAT3', 'VISSPAT4', 'VISSPAT6', 'VISSPAT7', 'VISSPAT8',       'PLAN1', 'PLAN2', 'PLAN3', 'PLAN4', 'PLAN5', 'ORGAN1', 'ORGAN2',       'ORGAN3', 'ORGAN4', 'ORGAN5', 'ORGAN6', 'DIVATT1', 'DIVATT2', 'DIVATT3',       'DIVATT4']\n",
    "ecogsp = pd.read_csv(\"ADNI\\\\Raw_Data\\\\Assessment\\\\ECOGSP.csv\",index_col='RID', usecols=cols['ecogsp'])\n",
    "\n",
    "list_memory = [ 'MEMORY1', 'MEMORY2', 'MEMORY3', 'MEMORY4', 'MEMORY5',       'MEMORY6', 'MEMORY7', 'MEMORY8']\n",
    "list_lang = ['LANG1', 'LANG2', 'LANG3', 'LANG4',       'LANG5', 'LANG6', 'LANG7', 'LANG8', 'LANG9']\n",
    "list_vis = ['VISSPAT1', 'VISSPAT2',       'VISSPAT3', 'VISSPAT4', 'VISSPAT6', 'VISSPAT7', 'VISSPAT8']\n",
    "list_plan = ['PLAN1', 'PLAN2', 'PLAN3', 'PLAN4', 'PLAN5']\n",
    "list_org = ['ORGAN1', 'ORGAN2',       'ORGAN3', 'ORGAN4', 'ORGAN5', 'ORGAN6']\n",
    "list_div = ['DIVATT1', 'DIVATT2', 'DIVATT3',       'DIVATT4']\n",
    "\n",
    "ecogsp_new = pd.DataFrame()\n",
    "ecogsp1 = ecogsp.copy(deep = True)\n",
    "print(ecogsp1.shape)\n",
    "ecogsp = ecogsp[ecogsp['VISCODE2'].isin(['bl','m12','m06']) ]  \n",
    "ecogsp = ecogsp.reset_index().set_index(['RID','VISCODE2'])\n",
    "ecogsp = ecogsp[~ecogsp.index.duplicated()]\n",
    "ecogsp = ecogsp[ (ecogsp.isnull().sum(axis = 1) <= 30) ]\n",
    "ecogsp = ecogsp.unstack()\n",
    "ecogsp = ecogsp[ (ecogsp.isnull().sum(axis = 1) < 41) ]\n",
    "new_col_list_ecogsp = ecogsp.columns.levels[0]\n",
    "for a in new_col_list_ecogsp: \n",
    "    ecogsp[a] = ecogsp[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "    \n",
    "ecogsp = ecogsp.replace({9: None })\n",
    "ecogsp = ecogsp.T\n",
    "ecogsp_bl = ecogsp[ecogsp.index.get_level_values(1).isin(['bl'])]\n",
    "ecogsp_m06 = ecogsp[ecogsp.index.get_level_values(1).isin(['m06'])]\n",
    "ecogsp_m12 = ecogsp[ecogsp.index.get_level_values(1).isin(['m12'])]\n",
    " \n",
    "ecogsp_new['memory___bl']  =  ecogsp_bl[ecogsp_bl.index.get_level_values(0).isin(list_memory)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['lang___bl']  =  ecogsp_bl[ecogsp_bl.index.get_level_values(0).isin(list_lang)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['vis___bl']  =  ecogsp_bl[ecogsp_bl.index.get_level_values(0).isin(list_vis)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['plan___bl']  =  ecogsp_bl[ecogsp_bl.index.get_level_values(0).isin(list_plan)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['org___bl']  =  ecogsp_bl[ecogsp_bl.index.get_level_values(0).isin(list_org)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['division___bl']  =  ecogsp_bl[ecogsp_bl.index.get_level_values(0).isin(list_div)].mean( axis = 0, skipna = True  )\n",
    "\n",
    "ecogsp_new['memory___m06']  =  ecogsp_m06[ecogsp_m06.index.get_level_values(0).isin(list_memory)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['lang___m06']  =  ecogsp_m06[ecogsp_m06.index.get_level_values(0).isin(list_lang)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['vis___m06']  =  ecogsp_m06[ecogsp_m06.index.get_level_values(0).isin(list_vis)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['plan___m06']  =  ecogsp_m06[ecogsp_m06.index.get_level_values(0).isin(list_plan)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['org___m06']  =  ecogsp_m06[ecogsp_m06.index.get_level_values(0).isin(list_org)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['division___m06']  =  ecogsp_m06[ecogsp_m06.index.get_level_values(0).isin(list_div)].mean( axis = 0, skipna = True  )\n",
    "\n",
    "ecogsp_new['memory___m12']  =  ecogsp_m12[ecogsp_m12.index.get_level_values(0).isin(list_memory)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['lang___m12']  =  ecogsp_m12[ecogsp_m12.index.get_level_values(0).isin(list_lang)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['vis___m12']  =  ecogsp_m12[ecogsp_m12.index.get_level_values(0).isin(list_vis)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['plan___m12']  =  ecogsp_m12[ecogsp_m12.index.get_level_values(0).isin(list_plan)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['org___m12']  =  ecogsp_m12[ecogsp_m12.index.get_level_values(0).isin(list_org)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['division___m12']  =  ecogsp_m12[ecogsp_m12.index.get_level_values(0).isin(list_div)].mean( axis = 0, skipna = True  )\n",
    "ecogsp = ecogsp.dropna(axis=0)\n",
    "# reducing index level\n",
    "ecogsp_ruf = ecogsp_new.T.reset_index()\n",
    "ecogsp_ruf.iloc[:,0]  = 'ecogsp_'  + ecogsp_ruf.iloc[:,0]\n",
    "ecogsp_ruf = ecogsp_ruf.set_index(['index'])\n",
    "ecogsp_ruf = ecogsp_ruf.T\n",
    "ecogsp_no_encoding = ecogsp_ruf\n",
    "\n",
    "## Hot encoding\n",
    "#ecogsp_name_list = list( ecogsp.columns )\n",
    "#ecogsp_empty = pd.DataFrame()\n",
    "#for i in range(len(ecogsp_name_list)):\n",
    "#    name = ecogsp_name_list[i]\n",
    "#    ecogsp_with_dummies = pd.get_dummies(ecogsp[name], sparse=True, drop_first=True, prefix=name)\n",
    "#    ecogsp_empty = pd.concat([ecogsp_empty,ecogsp_with_dummies] , axis = 1)\n",
    "#    \n",
    "ecogsp = ecogsp_no_encoding   \n",
    "ecogsp = ecogsp.dropna(axis=0)\n",
    "ecogsp.name = 'Everyday cognition - self'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************\n",
      "Datasets considered ['moca', 'neurobat', 'npi_all', 'mmse', 'geriatric', 'ecogsp', 'ecogpt', 'cdr', 'faq'] :\n",
      "************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# datasets and visits of interest\n",
    "# dict_datasets dictionary will contain all the data sets.\n",
    "# common_rids is a list of all the common ridsin the datset\n",
    "datasets_of_interest = ['moca', 'neurobat','npi_all', 'mmse', 'geriatric', 'ecogsp', 'ecogpt', 'cdr' , 'faq' ]\n",
    "dict_datasets = {}\n",
    "dict_datasets['moca'] = moca    \n",
    "dict_datasets['neurobat'] = neurobat    \n",
    "dict_datasets['npi_all'] = npi_all\n",
    "dict_datasets['mmse'] = mmse             \n",
    "dict_datasets['geriatric'] = geriatric   \n",
    "dict_datasets['ecogsp'] = ecogsp\n",
    "#dict_datasets['UWNPSYCHSUM_10_27_17'] = uwn\n",
    "dict_datasets['ecogpt'] = ecogpt\n",
    "dict_datasets['cdr'] = cdr\n",
    "dict_datasets['faq'] = faq \n",
    "\n",
    "\n",
    "size_matrix = pd.DataFrame(np.zeros((len(dict_datasets),2)) )\n",
    "for r in range(len(dict_datasets) ):\n",
    "    size_matrix.iloc[r,0] = datasets_of_interest[r]\n",
    "    size_matrix.iloc[r,1] = len(dict_datasets[datasets_of_interest[r]])\n",
    "\n",
    "size_matrix.columns = ['dataset','count']\n",
    "size_matrix =size_matrix.set_index('dataset')\n",
    "size_matrix = size_matrix.sort_values(by = ['count'],ascending= False )\n",
    "             \n",
    "sorted_cols = list(size_matrix.index)\n",
    "common_rids = pd.DataFrame(np.zeros((len(dict_datasets),len(dict_datasets))))\n",
    "common_rids.columns = sorted_cols\n",
    "common_rids.index = sorted_cols\n",
    "\n",
    "for i in range(len(dict_datasets)):\n",
    "    for u in range(len(dict_datasets)):\n",
    "        if (u>=i):\n",
    "            a = list(dict_datasets[sorted_cols[i]].index)\n",
    "            b = list(dict_datasets[sorted_cols[u]].index)\n",
    "        \n",
    "            common = list(set(a).intersection(b))\n",
    "            common_rids.iloc[i,u] = len(common)\n",
    "\n",
    "Max_intersection_dataset_item = ['moca', 'neurobat','npi_all', 'mmse', 'geriatric', 'ecogsp', 'ecogpt', 'cdr' , 'faq' ]\n",
    "\n",
    "print('************************')\n",
    "print('Datasets considered {} :'.format(Max_intersection_dataset_item)  )\n",
    "print('************************')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to join data and do NMF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_data(Max_intersection_dataset, visit, list_months_to_be_considered):\n",
    "    patno_filtered_visited = dict_datasets[Max_intersection_dataset[0]]\n",
    "\n",
    "    for t in range(len(Max_intersection_dataset)-1):\n",
    "        patients = dict_datasets[Max_intersection_dataset[t+1]]\n",
    "        patno_filtered_visited = pd.merge(patno_filtered_visited, patients, left_index = True, right_index = True, how='inner')\n",
    "        \n",
    "    M_chosen = normalize(patno_filtered_visited,'m')\n",
    "    M_chosen = M_chosen.T[ M_chosen.T.isnull().sum(axis = 1)== 0 ].T\n",
    "    print(M_chosen.shape)\n",
    "    M_W_columns = ['PCA_1', 'PCA_2', 'PCA_3', 'PCA_2_1', 'PCA_2_2','ICA_1', 'ICA_2', 'NMF_2_1', 'NMF_2_2', \n",
    "               'NMF_3_1', 'NMF_3_2', 'NMF_3_3','ICA_3_1', 'ICA_3_2', 'ICA_3_3']\n",
    "    M_W = pd.DataFrame(index=M_chosen.index, columns=M_W_columns)\n",
    "    \n",
    "    # PCA\n",
    "    model_pca = sklearnPCA(n_components=3)\n",
    "    M_W[['PCA_1', 'PCA_2', 'PCA_3']] = model_pca.fit_transform(M_chosen)\n",
    "    model_pca = sklearnPCA(n_components=2)\n",
    "    M_W[['PCA_2_1', 'PCA_2_2']] = model_pca.fit_transform(M_chosen)\n",
    "\n",
    "    # ICA\n",
    "    model_ICA = decomposition.FastICA(n_components=2)\n",
    "    M_W[['ICA_1', 'ICA_2']] = model_ICA.fit_transform(M_chosen)\n",
    "    model_ICA = decomposition.FastICA(n_components=3)\n",
    "    M_W[['ICA_3_1', 'ICA_3_2', 'ICA_3_3']] = model_ICA.fit_transform(M_chosen)\n",
    "\n",
    "    # NMF\n",
    "    model_NMF = decomposition.NMF(n_components=2, init='nndsvda', max_iter=200)\n",
    "    model_NMF3 = decomposition.NMF(n_components=3, init='nndsvda', max_iter=200)\n",
    "    M_W[['NMF_2_1', 'NMF_2_2']] = model_NMF.fit_transform(M_chosen)\n",
    "    M_W[['NMF_3_1', 'NMF_3_2', 'NMF_3_3']] = model_NMF3.fit_transform(M_chosen)\n",
    "    \n",
    "    H = model_NMF.components_\n",
    "    H_columns = M_chosen.columns\n",
    "    M_H = pd.DataFrame(columns=H_columns)\n",
    "    M_H.loc[0] = H[0,:]\n",
    "    M_H.loc[1] = H[1,:]\n",
    "    M_H_T = M_H.T.sort_values(by=[1],ascending=False)\n",
    "    M_H_T3 = M_H_T\n",
    "    M_H_T.columns = ['axis 1','axis 2']\n",
    "    M_H_T = pd.DataFrame(M_H_T)\n",
    "    M_H_T = M_H_T.div(M_H_T.sum(axis=1), axis=0)\n",
    "    M_H_T['new'] = 0\n",
    "    M_H_T['new'] = M_H_T.apply(lambda M_H_T :  'axis 1' if (M_H_T['axis 1']> M_H_T['axis 2']+(M_H_T['axis 2'] *0.5) ) else 'axis 2' if (M_H_T['axis 2'] > M_H_T['axis 1'] +(M_H_T['axis 1']*0.5) ) else 'ambigious', axis=1)\n",
    "    M_H_T2 = M_H_T\n",
    "    M_H_T.to_csv(address + \"all_2d_list.csv\") \n",
    "    \n",
    "    H = model_NMF3.components_\n",
    "    H_columns = M_chosen.columns\n",
    "    M_H = pd.DataFrame(columns=H_columns)\n",
    "    M_H.loc[0] = H[0,:]\n",
    "    M_H.loc[1] = H[1,:]\n",
    "    M_H.loc[2] = H[2,:]\n",
    "    M_H_T = M_H.T.sort_values(by=[2],ascending=False)\n",
    "    M_H_T.columns = ['axis 1','axis 2', 'axis 3']\n",
    "    M_H_T = pd.DataFrame(M_H_T)\n",
    "    M_H_T = M_H_T.div(M_H_T.sum(axis=1), axis=0)\n",
    "    M_H_T['new'] = 0\n",
    "    M_H_T['new'] = M_H_T.apply(lambda M_H_T :  'axis 1' if (M_H_T['axis 1']> M_H_T['axis 2']+M_H_T['axis 3'] ) else 'axis 2' if (M_H_T['axis 2'] > M_H_T['axis 1'] +M_H_T['axis 3']) else 'axis 3' if (M_H_T['axis 3'] > M_H_T['axis 1'] +M_H_T['axis 2']) else 'ambigious'  , axis=1)\n",
    "    M_H_T.to_csv(address + \"all_3d_list.csv\") \n",
    "    M_H_T3 = M_H_T\n",
    "    redued_data = pd.DataFrame(M_W) # this datset contains all the ICA, PCA and NMF vectors\n",
    "    # plot the dimension reduction color makrked with participants' \"categories\", and \"gender\"\n",
    "    dignosis = pd.read_csv(\"ADNI\\\\Raw_Data\\\\Assessment\\\\dxsum.csv\",  usecols= ['RID','DXCHANGE','DXMDUE','DXCONFID','VISCODE'])\n",
    "    dignosis = dignosis[ ~(dignosis['DXCHANGE'].isnull())]\n",
    "    dignosis = dignosis[ ~(dignosis['DXMDUE'] == 'MCI due to other etiology')]\n",
    "    dignosis = dignosis[ ~(dignosis['DXCONFID'] == 'Mildly Confident')]\n",
    "    dignosis = dignosis[ ~(dignosis['DXCONFID'] == 'Uncertain')]\n",
    "    dignosis = dignosis[dignosis['RID'].isin(redued_data.index)]\n",
    "    dignosis = dignosis.set_index('RID')\n",
    "    dignosis = dignosis[dignosis['VISCODE'] == visit]\n",
    "    redued = redued_data.merge(dignosis, how = 'inner', left_index = True, right_index = True)\n",
    "    redued = redued[ ~(redued['DXCHANGE'].isnull())]   \n",
    "    redued.DXCHANGE = redued.DXCHANGE.replace(['Stable: NL to NL', 'Stable: NL','Stable: MCI','Stable: MCI to MCI',                                               'Stable: Dementia', 'Stable: Dementia to Dementia',                                               'Conversion: NL to MCI','Conversion: MCI to Dementia','Conversion: NL to Dementia',                                               'Reversion: MCI to NL', 'Reversion: Dementia to MCI'],[1,1,2,2,3,3,4,5,6,7,8])\n",
    "    # Replacing the codes as described earlier\n",
    "    redued.DXCHANGE = redued.DXCHANGE.replace([1,2,3,4,5,6,7,8],[1,2,3,2,3,3,1,2]) \n",
    "    colors_categories = redued.DXCHANGE.replace([1,2,3], ['red', 'blue', 'green'])\n",
    "    # use this or above 2 lines\n",
    "    return redued, colors_categories, M_chosen,dignosis,M_H_T2, M_H_T3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(572, 206)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "redued_item_24, colors_categories_item_24, M_chosen_item_24,dignosis,M_H_T2, M_H_T3 = project_data(Max_intersection_dataset_item, visits, list_months_to_be_considered)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Get Low medium and high cluster using GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    60\n",
      "2    44\n",
      "1    31\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "M_mci_dem = redued_item_24\n",
    "M_mci_dem_nmf_all = M_mci_dem[['NMF_2_1','NMF_2_2','NMF_3_1', 'NMF_3_2','NMF_3_3']].copy()\n",
    "M_mci_dem_nmf = M_mci_dem[['NMF_2_1', 'NMF_2_2']]\n",
    "M_mci_dem_nmf_proj_all = M_mci_dem_nmf_all[~(redued_item_24.DXCHANGE.isin([1]) )] # removing controls\n",
    "M_mci_dem_nmf_proj_3d_only = M_mci_dem_nmf_proj_all[['NMF_3_1','NMF_3_2','NMF_3_3']]\n",
    "M_mci_dem_nmf_proj = M_mci_dem_nmf_proj_all[['NMF_2_1','NMF_2_2']]\n",
    "\n",
    "try:\n",
    "    colors_categories_item_24_no_controls = redued_item_24[~(redued_item_24.DXCHANGE.isin([1]) )]['DXCHANGE'].replace([1,2,3], ['red', 'blue', 'green'])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def organize_prediction_moca(M_mci_dem_nmf_proj_3d_only,Predict_gmm):\n",
    "    M_mci_dem_nmf_proj = M_mci_dem_nmf_proj_3d_only\n",
    "    M_mci_dem_nmf_proj['predicted'] = Predict_gmm\n",
    "    a = list(pd.unique(Predict_gmm.iloc[:,0]))\n",
    "    srt = np.empty((len(a),2))\n",
    "    for i in a:\n",
    "        a = M_mci_dem_nmf_proj[M_mci_dem_nmf_proj.predicted == i].iloc[:,1].sum() / len(M_mci_dem_nmf_proj[M_mci_dem_nmf_proj.predicted == i])\n",
    "        #b =  M_mci_dem_nmf_proj[M_mci_dem_nmf_proj.predicted == i].iloc[:,1].sum() / len(M_mci_dem_nmf_proj[M_mci_dem_nmf_proj.predicted == i])\n",
    "        #c =  M_mci_dem_nmf_proj[M_mci_dem_nmf_proj.predicted == i].iloc[:,0].sum() / len(M_mci_dem_nmf_proj[M_mci_dem_nmf_proj.predicted == i])\n",
    "        srt[i,1] = a\n",
    "        srt[i,0] = i\n",
    "    srt = pd.DataFrame(srt).sort_values([1])\n",
    "    Predict_gmm.replace([srt.iloc[0,0],srt.iloc[1,0], srt.iloc[2,0] ],[0,1,2], inplace=True)   \n",
    "    return pd.DataFrame(Predict_gmm)\n",
    "\n",
    "\n",
    "from sklearn import mixture\n",
    "model_gmm = mixture.GaussianMixture(n_components=3, covariance_type='diag',  random_state = 0)\n",
    "model_gmm.fit(M_mci_dem_nmf_proj) # print(gmm.means_)\n",
    "# label the predicted and only keep HC and PDs\n",
    "Predict_gmm = pd.DataFrame(model_gmm.predict(M_mci_dem_nmf_proj))\n",
    "print(Predict_gmm.iloc[:,0].value_counts())\n",
    "Predict_gmm.columns = ['predicted']\n",
    "Predict_gmm.index = M_mci_dem_nmf_proj.index\n",
    "Predict_gmm = organize_prediction_moca(M_mci_dem_nmf_proj,Predict_gmm)\n",
    "M_mci_dem_nmf_proj['predicted'] = Predict_gmm\n",
    "#plot_side_by_side_2d(M_mci_dem_nmf_proj,Predict_gmm,redued_item_24,colors_categories_item_24,'item24','gmm')    \n",
    "nl_data = M_mci_dem_nmf[(redued_item_24.DXCHANGE.isin([1]) )]\n",
    "data_prediction_labels = pd.concat([nl_data,M_mci_dem_nmf_proj]).fillna(3)\n",
    "\n",
    "cols['apoe4'] = ['RID' , 'VISCODE' , 'APOE4'   ]\n",
    "apoe4 = pd.read_csv(\"C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\Raw_Data\\\\Assessment\\\\apoe4\\\\ADNIMERGE.csv\",index_col='RID', usecols=cols['apoe4'])\n",
    "apoe4['VISCODE2'] = apoe4['VISCODE']\n",
    "apoe4['VISCODE'].value_counts()\n",
    "del apoe4['VISCODE']\n",
    "apoe4 = apoe4[apoe4['VISCODE2'].isin([visits]) ] \n",
    "Predict_gmm = data_prediction_labels\n",
    "redued = Predict_gmm.merge(apoe4['APOE4'].to_frame(), left_index=True, right_index=True)\n",
    "redued = redued.merge(redued_item_24['DXCHANGE'].to_frame(), left_index=True, right_index=True)\n",
    "\n",
    "redued['predicted'] = redued['predicted'].replace([0,1,2,3],['Low','Moderate','High','Controls'])\n",
    "redued['DXCHANGE'] = redued['DXCHANGE'].replace([1,2,3],['Controls','MCI','Dementia'])\n",
    "\n",
    "\n",
    "redued_2 = redued[['NMF_2_1','predicted']]\n",
    "redued_2['Progression Vector'] = 1\n",
    "redued_2.columns = ['val','predicted','Progression Vector']\n",
    "redued_1 = redued[['NMF_2_2','predicted']]\n",
    "redued_1['Progression Vector'] = 2\n",
    "redued_1.columns = ['val','predicted','Progression Vector']\n",
    "redued_new = pd.concat([redued_1,redued_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'predicted' in list(M_chosen_item_24.columns):\n",
    "    print('Labels in train dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(244, 206)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NMF_2_1</th>\n",
       "      <th>NMF_2_2</th>\n",
       "      <th>predicted</th>\n",
       "      <th>APOE4</th>\n",
       "      <th>DXCHANGE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4004</th>\n",
       "      <td>0.466317</td>\n",
       "      <td>0.166003</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4005</th>\n",
       "      <td>0.278958</td>\n",
       "      <td>0.438798</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4012</th>\n",
       "      <td>0.431301</td>\n",
       "      <td>0.146465</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4014</th>\n",
       "      <td>0.338205</td>\n",
       "      <td>0.274495</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Controls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4015</th>\n",
       "      <td>0.231796</td>\n",
       "      <td>0.462503</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dementia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       NMF_2_1   NMF_2_2  predicted  APOE4  DXCHANGE\n",
       "RID                                                 \n",
       "4004  0.466317  0.166003          2    0.0       MCI\n",
       "4005  0.278958  0.438798          3    1.0       MCI\n",
       "4012  0.431301  0.146465          2    0.0       MCI\n",
       "4014  0.338205  0.274495          0    0.0  Controls\n",
       "4015  0.231796  0.462503          3    1.0  Dementia"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = M_chosen_item_24.dropna().index\n",
    "index2 = redued.dropna().index\n",
    "\n",
    "ind = set(index).intersection(set(index2) )\n",
    "\n",
    "redued['predicted'] = redued['predicted'].replace(['Controls','Low','Moderate','High'],[0,1,2,3])\n",
    "M_chosen_item_24 = M_chosen_item_24.loc[ind,:].sort_index()\n",
    "print(M_chosen_item_24.shape)\n",
    "redued = redued.loc[ind,:].sort_index()\n",
    "redued.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data for 5 fold cross validation and store it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the 1 value\n",
      "for the 2 value\n",
      "for the 3 value\n",
      "for the 4 value\n",
      "for the 5 value\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(  M_chosen_item_24 ,  redued['predicted']   , test_size=0.2, random_state=i)\n",
    "    \n",
    "    x_train.to_csv('C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\sem4\\\\adni_lstm_predict_label_data\\\\progression_labels\\\\split_'+str(i)+'_'+visits+'\\\\train_data.csv' )\n",
    "    x_test.to_csv('C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\sem4\\\\adni_lstm_predict_label_data\\\\progression_labels\\\\split_'+str(i)+'_'+visits+'\\\\test_data.csv')\n",
    "\n",
    "    y_train.to_csv('C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\sem4\\\\adni_lstm_predict_label_data\\\\progression_labels\\\\split_'+str(i)+'_'+visits +'\\\\train_labels.csv')\n",
    "    y_test.to_csv('C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\sem4\\\\adni_lstm_predict_label_data\\\\progression_labels\\\\split_'+str(i)+'_'+visits +'\\\\test_labels.csv')\n",
    "    print('for the {} value'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split number 1\n",
      "Split number 2\n",
      "Split number 3\n",
      "Split number 4\n",
      "Split number 5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "address = 'C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\sem4\\\\adni_lstm_predict_label_data\\\\progression_labels\\\\split_'+str(i)+'_'+visits+'\\\\'\n",
    "os.chdir(address)\n",
    "\n",
    "# this will contain a dictionary. which will have 2  index named 'label' and 'data'\n",
    "def convert_data(data, data_label):\n",
    "    data_dict = {}\n",
    "    i = 0\n",
    "    for a in list( data.index ):\n",
    "        # creating empty dictionary to save data and label.\n",
    "        dict1 = {}\n",
    "        # Defining new data frame as per format\n",
    "        data_1 = pd.DataFrame(index = unique_cols, columns = ['bl','m06','m12'] )\n",
    "        for g in data.columns:\n",
    "            data_1.loc[g.split('___')[0], g.split('___')[1]] = data.loc[a,g]\n",
    "        # Checking if there are more than 3 null value\n",
    "        if max( data_1.isnull().sum(axis = 1) ) > 2:\n",
    "            print('error more null values {} '.format(data_1.isnull().sum(axis = 1)))\n",
    "            print(a)\n",
    "            print(data_1.head() )\n",
    "        # Filling nulls in row with average of other.\n",
    "        # in many cases data was only collected for say 'bl' and 'm06'\n",
    "        data_1 = data_1.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "        if data_1.isnull().sum(axis = 1).sum(axis = 0)  != 0:\n",
    "            print('null value in data')    \n",
    "        dict1['data'] = data_1\n",
    "        dict1['label'] = int( data_label.T[a] )\n",
    "        dict1['index'] = a\n",
    "        data_dict[i] = dict1\n",
    "        i += 1\n",
    "    return(data_dict)\n",
    "    \n",
    "# Reading raw data  \n",
    "for i in range(1,6):\n",
    "    print('Split number '+str(i))\n",
    "    address = 'C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\sem4\\\\adni_lstm_predict_label_data\\\\progression_labels\\\\split_'+str(i)+'_'+visits+'\\\\'\n",
    "    os.chdir(address)\n",
    "    data_test = pd.read_csv(address+'test_data.csv', index_col = 0)\n",
    "    data_label_test = pd.read_csv(address+'test_labels.csv', index_col = 0, header = None )\n",
    "    data = pd.read_csv(address+'train_data.csv', index_col = 0)\n",
    "    data_label = pd.read_csv(address+'train_labels.csv', index_col = 0, header = None )\n",
    "    list_columns = [ a.split('___')[0] for a in list( data.columns )     ]\n",
    "    unique_cols = list( set(list_columns) )\n",
    "\n",
    "\n",
    "    # converting raw data\n",
    "    data_dict = convert_data(data, data_label)\n",
    "    data_dict_test = convert_data(data_test, data_label_test)\n",
    "\n",
    "\n",
    "    # saving it as numpy object\n",
    "    with open('data_dict_test.pickle', 'wb') as handle:\n",
    "        pickle.dump(data_dict_test , handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('data_dict_test.pickle', 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "\n",
    "    with open('data_dict.pickle', 'wb') as handle:\n",
    "        pickle.dump(data_dict , handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('data_dict.pickle', 'rb') as handle:\n",
    "        a = pickle.load(handle)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################\n",
    "### Check if there are 3 null values in a row.\n",
    "#for a in data_dict:\n",
    "#    if max( data_dict[a]['data'].isnull().sum(axis = 1) ) > 2:\n",
    "#        print(a)\n",
    "#        print(max( data_dict[a]['data'].isnull().sum(axis = 1) ) > 2)\n",
    "#    else:\n",
    "#        pass   \n",
    "###############################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting split 1\n",
      "Test and Train Accuracy for 1: 79.59183673469387 and 100.0 %\n",
      "----END----\n",
      "\n",
      " Starting split 2\n",
      "Test and Train Accuracy for 2: 77.55102040816327 and 100.0 %\n",
      "----END----\n",
      "\n",
      " Starting split 3\n",
      "Test and Train Accuracy for 3: 73.46938775510205 and 100.0 %\n",
      "----END----\n",
      "\n",
      " Starting split 4\n",
      "Test and Train Accuracy for 4: 73.46938775510205 and 100.0 %\n",
      "----END----\n",
      "\n",
      " Starting split 5\n",
      "Test and Train Accuracy for 5: 75.51020408163265 and 100.0 %\n",
      "----END----\n",
      "Five fold cross validation average accuracy for prediction at visit m48  is 75.91836734693877 \n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jan 22 17:45:32 2019\n",
    "\n",
    "@author: Vipul Satone\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# NL = 0\n",
    "# MCI = 1\n",
    "# DEM = 2\n",
    "\n",
    "# Setting up random seed\n",
    "torch.manual_seed(12)\n",
    "torch.cuda.manual_seed(12)\n",
    "np.random.seed(12)\n",
    "random.seed(12)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "root = 'C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\sem4\\\\adni_lstm_predict_label_data\\\\progression_labels'\n",
    "\n",
    "os.chdir(root)\n",
    "\n",
    "class adniDataloader(torchUtils.Dataset):\n",
    "    def __init__(self, root, data_path):\n",
    "        self.root = root\n",
    "        with open(data_path, 'rb') as handle:\n",
    "            self.data = pickle.load(handle)\n",
    "        self.len = len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
    "#        print(index)\n",
    "        data_dict = self.data\n",
    "        image = np.array(data_dict[index]['data'])\n",
    "        target = np.array(data_dict[index]['label'])\n",
    "        return torch.tensor(image, dtype=torch.float, device=device), target\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Bidirectional recurrent neural network (many-to-one)\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        np.random.seed(1)\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True,dropout=dropout_per,bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)  # 2 for bidirection\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial states\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) # 2 for bidirection \n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "def train_test(data_path,data_path_test, split):\n",
    "    dataset = adniDataloader(root,data_path)\n",
    "    train_loader = torchUtils.DataLoader(dataset=dataset, \n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=True)\n",
    "    \n",
    "    model = BiRNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "    \n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            \n",
    "\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "#            if (i+1) % 1 == 0:\n",
    "#                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "#                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "    _, predicted_on_train = torch.max(outputs.data, 1)\n",
    "\n",
    "    train_acc = 100 * ( predicted_on_train.numpy() == labels.numpy() ).sum().item() / len(predicted_on_train.numpy())\n",
    "    \n",
    "    \n",
    "    \n",
    "    dataset = adniDataloader(root,data_path_test)\n",
    "    test_loader = torchUtils.DataLoader(dataset=dataset, \n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=True)\n",
    "    \n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    # Test the model\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.long()).sum().item()\n",
    "            list1.extend(predicted.numpy())\n",
    "            list2.extend(labels.numpy() )\n",
    "            test_acc = 100 * correct / total\n",
    "    \n",
    "        print('Test and Train Accuracy for {}: {} and {} %'.format( split, test_acc, train_acc) ) \n",
    "    \n",
    "    \n",
    "#    print('For {} split'.format(split) )\n",
    "#    solution = pd.DataFrame( np.column_stack((list1, list2 )) )\n",
    "#    solution = solution.replace([0,1,2],['NL','MCI','Dem'])\n",
    "#    \n",
    "#    solution['result'] = solution.apply(lambda x : str(x[1])+'->'+ str(x[0]) if x[0] != x[1]  else \"\", axis=1)\n",
    "#    print( solution['result'].value_counts() )\n",
    "    print('----END----')\n",
    "    return(test_acc)\n",
    "\n",
    "# Hyper-parameters\n",
    "sequence_length = 3\n",
    "input_size = 79\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "num_classes = 4\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "batch_size = 20\n",
    "dropout_per = 0.2\n",
    "\n",
    "total = 0\n",
    "for split in range(1,6):\n",
    "    print('\\n Starting split {}'.format(split))\n",
    "    data_path = 'C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\sem4\\\\adni_lstm_predict_label_data\\\\progression_labels\\\\split_'+str(i)+'_'+visits+'\\\\data_dict.pickle'\n",
    "    data_path_test = 'C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\sem4\\\\adni_lstm_predict_label_data\\\\progression_labels\\\\split_'+str(i)+'_'+visits+'\\\\data_dict_test.pickle'\n",
    "    acc = train_test(data_path,data_path_test, split)\n",
    "    total += acc\n",
    "print('Five fold cross validation average accuracy for prediction at visit {}  is {} '.format(visits, (total /split) ) )\n",
    "\n",
    "#with open('solution.pickle', 'wb') as handle:\n",
    "#    pickle.dump(solution , handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#\n",
    "#with open('solution.pickle', 'rb') as handle:\n",
    "#    a = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of control, low, medium and high progression rate using LSTM\n",
    "#### Predictions are made using bl, m06 and m12 dataset. Control, Low, Medium and High class is predicted at m24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already present\n",
      "Directory already present\n",
      "Directory already present\n",
      "Directory already present\n",
      "Directory already present\n",
      "Directory already present\n",
      "Directory already present\n",
      "Directory already present\n",
      "Directory already present\n",
      "Directory already present\n",
      "Directory already present\n",
      "Directory already present\n"
     ]
    }
   ],
   "source": [
    "#Keeping the directory correct\n",
    "import os\n",
    "os.chdir(\"C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\sem4\\\\\") # default directory\n",
    "\n",
    "# Create directoy to save processed data and results\n",
    "if not os.path.exists('adni_lstm_predict_label_data'):\n",
    "    os.makedirs('adni_lstm_predict_label_data')\n",
    "else:\n",
    "    print('Directory already present')\n",
    "    \n",
    "# Create directoy to save processed data and results\n",
    "if not os.path.exists('adni_lstm_predict_label_data\\\\progression_labels'):\n",
    "    os.makedirs('adni_lstm_predict_label_data\\\\progression_labels')\n",
    "else:\n",
    "    print('Directory already present')\n",
    "    \n",
    "for i in range(1,6):\n",
    "    if not os.path.exists('adni_lstm_predict_label_data\\\\progression_labels\\\\split_' + str(i) + '_m24'):\n",
    "        os.makedirs('adni_lstm_predict_label_data\\\\progression_labels\\\\split_' + str(i) + '_m24')\n",
    "    else:\n",
    "        print('Directory already present')\n",
    "\n",
    "        \n",
    "for i in range(1,6):\n",
    "    if not os.path.exists('adni_lstm_predict_label_data\\\\progression_labels\\\\split_' + str(i) + '_m48'):\n",
    "        os.makedirs('adni_lstm_predict_label_data\\\\progression_labels\\\\split_' + str(i) + '_m48')\n",
    "    else:\n",
    "        print('Directory already present')\n",
    "        \n",
    "os.chdir('C:\\\\Users\\\\Vipul Satone\\\\health data') # default directory\n",
    "address = \"C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\sem4\\\\\" # directory where images are to be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assessment data\n",
    "cols = {}\n",
    "list_months_to_be_considered = ['bl','m06','m12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = 'm24'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Following function gives information about dataset to be imputed.\n",
    "def data_info(dataset):\n",
    "    print('Name of dataset is: ' + dataset.name) \n",
    "    print('\\n0th level of columns is ')\n",
    "    print(list(pd.Series(dataset.columns.get_level_values(0)).unique()) )\n",
    "    print('\\n1st level of columns is: ')\n",
    "    print(list(pd.Series(dataset.columns.get_level_values(1)).unique()) )\n",
    "    print('\\nShape of datset is:')\n",
    "    print(dataset.shape)\n",
    "    print('\\nTotal number of missing values: ')\n",
    "    print(dataset.isnull().sum().sum())\n",
    "    \n",
    "    \n",
    "\n",
    "# Argument Train1 is dta to be normalized. IF argument b is 'z' the z normalization is done otherwise minmax normalization is done.\n",
    "def normalize(Train1,b):\n",
    "    col_names = list(Train1.columns)\n",
    "    Train1 = pd.DataFrame(Train1)\n",
    "    if (b == 'z'):\n",
    "        for i in range(Train1.shape[1]):\n",
    "            Train1[col_names[i]] = (Train1[col_names[i]] - Train1[col_names[i]].mean(skipna = True)) / Train1[col_names[i]].std(skipna = True)\n",
    "    else:\n",
    "        for i in range(Train1.shape[1]):\n",
    "            Train1[col_names[i]] = (Train1[col_names[i]] - min(Train1[col_names[i]]) )/ ( max(Train1[col_names[i]] ) - min(Train1[col_names[i]]) )\n",
    "    return Train1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10414, 7)\n"
     ]
    }
   ],
   "source": [
    "#CDR\n",
    "cols['cdr'] = ['RID','VISCODE2', 'CDMEMORY', 'CDORIENT', 'CDJUDGE' ,'CDCOMMUN' ,'CDHOME' ,'CDCARE']\n",
    "cdr = pd.read_csv(\"ADNI\\\\Raw_Data\\\\Assessment\\\\CDR.csv\",index_col='RID', usecols=cols['cdr'])\n",
    "cdr1 = cdr.copy(deep = True)\n",
    "print(cdr1.shape)\n",
    "cdr = cdr[cdr['VISCODE2'].isin(['bl','m12','m06']) ]  \n",
    "cdr = cdr.reset_index().set_index(['RID','VISCODE2'])\n",
    "cdr = cdr[~cdr.index.duplicated()].unstack()\n",
    "cdr = cdr[ (cdr.isnull().sum(axis = 1) <= 4) ]\n",
    "cdr = cdr.T\n",
    "cdr = cdr[cdr.index.get_level_values(1).isin(list_months_to_be_considered)].T\n",
    "# reducing index level\n",
    "cdr_ruf = cdr.T.reset_index()\n",
    "cdr_ruf.iloc[:,0]  = 'cdr__' + cdr_ruf.iloc[:,0] +  '___' + cdr_ruf.iloc[:,1]\n",
    "cdr_ruf = cdr_ruf.set_index('level_0')\n",
    "cdr = cdr_ruf.iloc[:,1:].T\n",
    "cdr_no_encoding = cdr\n",
    "\n",
    "cdr.name = 'Clinical Dementia Rating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12484, 48)\n"
     ]
    }
   ],
   "source": [
    "#NEUROBAT - Just using the total scores CLCOKSCOR, COPYSCOR, BNTTOTAL\n",
    "cols['neurobat'] = ['RID', 'VISCODE2', 'CLOCKSCOR', 'COPYSCOR', 'LMSTORY', 'LIMMTOTAL', 'LIMMEND',\n",
    "       'AVTOT1', 'AVERR1', 'AVTOT2', 'AVERR2', 'AVTOT3', 'AVERR3', 'AVTOT4',\n",
    "       'AVERR4', 'AVTOT5', 'AVERR5', 'AVTOT6', 'AVERR6', 'AVTOTB', 'AVERRB',\n",
    "       'AVENDED', 'DSPANFOR', 'DSPANFLTH', 'DSPANBAC', 'DSPANBLTH',\n",
    "       'CATANIMSC', 'CATANPERS', 'CATANINTR', 'CATVEGESC', 'CATVGPERS',\n",
    "       'CATVGINTR', 'TRAASCOR', 'TRAAERRCOM', 'TRAAERROM', 'TRABSCOR',\n",
    "       'TRABERRCOM', 'TRABERROM', 'DIGITSCOR', 'LDELBEGIN', 'LDELTOTAL',\n",
    "       'LDELCUE','BNTTOTAL', 'AVDELBEGAN', 'AVDEL30MIN', 'AVDELERR1',\n",
    "       'AVDELTOT', 'AVDELERR2', 'ANARTND', 'ANARTERR']\n",
    "neurobat_1 = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat'], index_col = ['RID', 'VISCODE2'])\n",
    "print(neurobat_1.shape)\n",
    "cols['neurobat_clock'] = ['RID', 'VISCODE2', 'CLOCKSCOR']\n",
    "neurobat_clock = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat_clock'], index_col = ['RID', 'VISCODE2'])\n",
    "neurobat_clock1 = neurobat_clock.copy(deep = True) \n",
    "neurobat_clock = neurobat_clock[~neurobat_clock.index.duplicated()].reset_index()\n",
    "neurobat_clock = neurobat_clock[neurobat_clock.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "neurobat_clock = neurobat_clock.unstack()\n",
    "neurobat_clock = neurobat_clock[ (neurobat_clock.isnull().sum(axis = 1) <= 1) ]\n",
    "new_col_list_neurobat_clock = neurobat_clock.columns.levels[0]\n",
    "for a in new_col_list_neurobat_clock: \n",
    "    neurobat_clock[a] = neurobat_clock[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "neurobat_clock.name = 'Neuropsychological Battery (subdata - clock)'\n",
    "#data_info(neurobat_clock)\n",
    "\n",
    "cols['neurobat_copy'] = ['RID', 'VISCODE2', 'COPYSCOR']\n",
    "neurobat_copy = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat_copy'], index_col = ['RID', 'VISCODE2'])\n",
    "neurobat_copy1 = neurobat_copy.copy(deep = True) \n",
    "neurobat_copy = neurobat_copy[~neurobat_copy.index.duplicated()].reset_index()\n",
    "neurobat_copy = neurobat_copy[neurobat_copy.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "neurobat_copy = neurobat_copy.unstack()\n",
    "neurobat_copy = neurobat_copy[ (neurobat_copy.isnull().sum(axis = 1) <= 1) ]\n",
    "new_col_list_neurobat_copy = neurobat_copy.columns.levels[0]\n",
    "for a in new_col_list_neurobat_copy: \n",
    "    neurobat_copy[a] = neurobat_copy[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "neurobat_copy.name = 'Neuropsychological Battery (subdata - copy)'\n",
    "#data_info(neurobat_copy)\n",
    "\n",
    "cols['neurobat_limm_story'] = ['RID', 'VISCODE2', 'LIMMTOTAL']\n",
    "neurobat_limm_story = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat_limm_story'], index_col = ['RID', 'VISCODE2'])\n",
    "neurobat_limm_story1 = neurobat_limm_story.copy(deep = True) \n",
    "neurobat_limm_story = neurobat_limm_story[~neurobat_limm_story.index.duplicated()].reset_index()\n",
    "neurobat_limm_story = neurobat_limm_story[neurobat_limm_story.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "#neurobat_clock = neurobat_clock[ (neurobat_clock.isnull().sum(axis = 1) <= 1) ]\n",
    "neurobat_limm_story = neurobat_limm_story.unstack()\n",
    "neurobat_limm_story = neurobat_limm_story.drop(['m06','bl'], axis=1, level=1)\n",
    "#neurobat_limm_story = neurobat_limm_story.T[ (neurobat_limm_story.columns.levels[1]) == 'm12' ].T\n",
    "neurobat_limm_story = neurobat_limm_story[ (neurobat_limm_story.isnull().sum(axis = 1) < 1) ]\n",
    "neurobat_limm_story.name = 'Neuropsychological Battery (subdata - story)'\n",
    "#data_info(neurobat_limm_story)\n",
    "\n",
    "cols['neurobat_dspan'] = ['RID', 'VISCODE2','DSPANFOR', 'DSPANFLTH', 'DSPANBAC', 'DSPANBLTH']\n",
    "neurobat_dspan = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat_dspan'], index_col = ['RID', 'VISCODE2'])\n",
    "neurobat_dspan1 = neurobat_dspan.copy(deep = True) \n",
    "neurobat_dspan = neurobat_dspan[~neurobat_dspan.index.duplicated()].reset_index()\n",
    "neurobat_dspan = neurobat_dspan[neurobat_dspan.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "neurobat_dspan = neurobat_dspan[ (neurobat_dspan.isnull().sum(axis = 1) < 4) ]\n",
    "neurobat_dspan = neurobat_dspan.unstack()\n",
    "neurobat_dspan = neurobat_dspan[ (neurobat_dspan.isnull().sum(axis = 1) <6) ]\n",
    "new_col_list_neurobat_dspan = neurobat_dspan.columns.levels[0]\n",
    "for a in new_col_list_neurobat_dspan: \n",
    "    neurobat_dspan[a] = neurobat_dspan[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "neurobat_dspan.name = 'Neuropsychological Battery (subdata - digit span)'\n",
    "#data_info(neurobat_dspan)\n",
    "\n",
    "cols['neurobat_cat_flu'] = ['RID', 'VISCODE2','CATANIMSC', 'CATANPERS', 'CATANINTR', 'CATVEGESC', 'CATVGPERS','CATVGINTR']\n",
    "neurobat_cat_flu = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat_cat_flu'], index_col = ['RID', 'VISCODE2'])\n",
    "neurobat_cat_flu1 = neurobat_cat_flu.copy(deep = True) \n",
    "neurobat_cat_flu = neurobat_cat_flu[~neurobat_cat_flu.index.duplicated()].reset_index()\n",
    "neurobat_cat_flu = neurobat_cat_flu[neurobat_cat_flu.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "neurobat_cat_flu = neurobat_cat_flu.replace({-1: np.NAN})\n",
    "neurobat_cat_flu = neurobat_cat_flu[ (neurobat_cat_flu.isnull().sum(axis = 1) < 4) ]\n",
    "del neurobat_cat_flu['CATVEGESC']\n",
    "del neurobat_cat_flu['CATVGPERS']\n",
    "del neurobat_cat_flu['CATVGINTR']\n",
    "neurobat_cat_flu = neurobat_cat_flu.unstack()\n",
    "neurobat_cat_flu = neurobat_cat_flu[ (neurobat_cat_flu.isnull().sum(axis = 1) <4) ]\n",
    "new_col_list_neurobat_cat_flu = neurobat_cat_flu.columns.levels[0]\n",
    "for a in new_col_list_neurobat_cat_flu: \n",
    "    neurobat_cat_flu[a] = neurobat_cat_flu[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "neurobat_cat_flu.name = 'Neuropsychological Battery (subdata - category fluency : only animal examples)'\n",
    "#data_info(neurobat_cat_flu)\n",
    "\n",
    "cols['neurobat_trail'] = ['RID', 'VISCODE2', 'TRAASCOR', 'TRAAERRCOM', 'TRAAERROM', 'TRABSCOR','TRABERRCOM', 'TRABERROM']\n",
    "neurobat_trail = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat_trail'], index_col = ['RID', 'VISCODE2'])\n",
    "neurobat_trail1 = neurobat_trail.copy(deep = True) \n",
    "neurobat_trail = neurobat_trail[~neurobat_trail.index.duplicated()].reset_index()\n",
    "neurobat_trail = neurobat_trail[neurobat_trail.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "neurobat_trail = neurobat_trail[ (neurobat_trail.isnull().sum(axis = 1) < 3) ]\n",
    "neurobat_trail = neurobat_trail.unstack()\n",
    "neurobat_trail = neurobat_trail[ (neurobat_trail.isnull().sum(axis = 1) <=6) ]\n",
    "new_col_list_neurobat_trail = neurobat_trail.columns.levels[0]\n",
    "for a in new_col_list_neurobat_trail: \n",
    "    neurobat_trail[a] = neurobat_trail[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "neurobat_trail.name = 'Neuropsychological Battery (subdata - Trail making)'\n",
    "#data_info(neurobat_trail)\n",
    "\n",
    "cols['neurobat_av'] = ['RID', 'VISCODE2','AVTOT1', 'AVDELERR1','AVDELTOT', 'AVERR1', 'AVTOT2', 'AVERR2', 'AVTOT3',     'AVERR3','AVDELERR2', 'AVTOT4','AVERR4', 'AVTOT5', 'AVERR5', 'AVTOT6', 'AVERR6', 'AVTOTB', 'AVERRB','AVDEL30MIN']\n",
    "neurobat_av = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat_av'], index_col = ['RID', 'VISCODE2'])\n",
    "neurobat_av1 = neurobat_av.copy(deep = True) \n",
    "neurobat_av = neurobat_av[~neurobat_av.index.duplicated()].reset_index()\n",
    "neurobat_av = neurobat_av[neurobat_av.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "neurobat_av = neurobat_av.unstack()\n",
    "neurobat_av = neurobat_av[ (neurobat_av.isnull().sum(axis = 1) <25) ]\n",
    "new_col_list_neurobat_av = neurobat_av.columns.levels[0]\n",
    "for a in new_col_list_neurobat_av: \n",
    "    neurobat_av[a] = neurobat_av[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "neurobat_av.name = 'Neuropsychological Battery (subdata - av)'\n",
    "#data_info(neurobat_av)\n",
    "\n",
    "cols['neurobat_digit_score'] = ['RID', 'VISCODE2','DIGITSCOR']\n",
    "neurobat_digit_score = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat_digit_score'], index_col = ['RID', 'VISCODE2'])\n",
    "neurobat_digit_score1 = neurobat_digit_score.copy(deep = True) \n",
    "neurobat_digit_score = neurobat_digit_score[~neurobat_digit_score.index.duplicated()].reset_index()\n",
    "neurobat_digit_score = neurobat_digit_score[neurobat_digit_score.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "neurobat_digit_score = neurobat_digit_score[ (neurobat_digit_score.isnull().sum(axis = 1) < 1) ]\n",
    "#neurobat_clock = neurobat_clock[ (neurobat_clock.isnull().sum(axis = 1) <= 1) ]\n",
    "neurobat_digit_score = neurobat_digit_score.unstack()\n",
    "neurobat_digit_score = neurobat_digit_score[ (neurobat_digit_score.isnull().sum(axis = 1) <=1) ]\n",
    "new_col_list_neurobat_digit_score = neurobat_digit_score.columns.levels[0]\n",
    "for a in new_col_list_neurobat_digit_score: \n",
    "    neurobat_digit_score[a] = neurobat_digit_score[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "neurobat_digit_score.name = 'Neuropsychological Battery (subdata - digit score)'\n",
    "#data_info(neurobat_digit_score)\n",
    "\n",
    "cols['neurobat_logical_memory'] = ['RID', 'VISCODE2','LDELTOTAL']\n",
    "neurobat_logical_memory = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat_logical_memory'], index_col = ['RID', 'VISCODE2'])\n",
    "neurobat_logical_memory1 = neurobat_logical_memory.copy(deep = True) \n",
    "neurobat_logical_memory = neurobat_logical_memory[~neurobat_logical_memory.index.duplicated()].reset_index()\n",
    "neurobat_logical_memory = neurobat_logical_memory[neurobat_logical_memory.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "neurobat_logical_memory = neurobat_logical_memory[ (neurobat_logical_memory.isnull().sum(axis = 1) < 1) ]\n",
    "neurobat_logical_memory = neurobat_logical_memory.unstack()\n",
    "neurobat_logical_memory.name = 'Neuropsychological Battery (subdata - logical memeory test)'\n",
    "#data_info(neurobat_logical_memory)\n",
    "\n",
    "cols['neurobat_boston_naming_test'] = ['RID', 'VISCODE2', 'BNTSPONT','BNTSTIM','BNTCSTIM','BNTPHON','BNTCPHON']\n",
    "neurobat_boston_naming_test = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat_boston_naming_test'], index_col = ['RID', 'VISCODE2'])\n",
    "neurobat_boston_naming_test1 = neurobat_boston_naming_test.copy(deep = True) \n",
    "neurobat_boston_naming_test = neurobat_boston_naming_test[~neurobat_boston_naming_test.index.duplicated()].reset_index()\n",
    "neurobat_boston_naming_test = neurobat_boston_naming_test[neurobat_boston_naming_test.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "neurobat_boston_naming_test = neurobat_boston_naming_test[ (neurobat_boston_naming_test.isnull().sum(axis = 1) < 5) ]\n",
    "neurobat_boston_naming_test = neurobat_boston_naming_test.unstack()\n",
    "neurobat_boston_naming_test = neurobat_boston_naming_test[ (neurobat_boston_naming_test.isnull().sum(axis = 1) <6) ]\n",
    "new_col_list_neurobat_boston_naming_test = neurobat_boston_naming_test.columns.levels[0]\n",
    "for a in new_col_list_neurobat_boston_naming_test: \n",
    "    neurobat_boston_naming_test[a] = neurobat_boston_naming_test[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "neurobat_boston_naming_test.name = 'Neuropsychological Battery (subdata - Boston naming test)'\n",
    "#data_info(neurobat_boston_naming_test)\n",
    "\n",
    "cols['neurobat_anrt'] = ['RID', 'VISCODE2', 'ANARTND']\n",
    "neurobat_anrt = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NEUROBAT.csv', usecols=cols['neurobat_anrt'], index_col = ['RID', 'VISCODE2'])\n",
    "neurobat_anrt1 = neurobat_anrt.copy(deep = True) \n",
    "neurobat_anrt = neurobat_anrt[~neurobat_anrt.index.duplicated()].reset_index()\n",
    "neurobat_anrt = neurobat_anrt[neurobat_anrt.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "neurobat_anrt = neurobat_anrt[ (neurobat_anrt.isnull().sum(axis = 1) < 1) ]\n",
    "#neurobat_clock = neurobat_clock[ (neurobat_clock.isnull().sum(axis = 1) <= 1) ]\n",
    "neurobat_anrt = neurobat_anrt.unstack()\n",
    "neurobat_anrt = neurobat_anrt[ (neurobat_anrt.isnull().sum(axis = 1) <=1) ]\n",
    "new_col_list_neurobat_anrt = neurobat_anrt.columns.levels[0]\n",
    "for a in new_col_list_neurobat_anrt: \n",
    "    neurobat_anrt[a] = neurobat_anrt[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "neurobat_anrt.name = 'Neuropsychological Battery (subdata - American national reading test)'\n",
    "#data_info(neurobat_anrt)\n",
    "\n",
    "neurobat1 = pd.merge(neurobat_clock,neurobat_copy , left_index = True, right_index = True, how='inner')\n",
    "neurobat1 = pd.merge(neurobat1,neurobat_limm_story , left_index = True, right_index = True, how='inner')\n",
    "neurobat1 = pd.merge(neurobat1,neurobat_av , left_index = True, right_index = True, how='inner')\n",
    "neurobat1 = pd.merge(neurobat1,neurobat_cat_flu , left_index = True, right_index = True, how='inner')\n",
    "neurobat1 = pd.merge(neurobat1,neurobat_trail , left_index = True, right_index = True, how='inner')\n",
    "neurobat1 = pd.merge(neurobat1,neurobat_logical_memory , left_index = True, right_index = True, how='inner')\n",
    "neurobat1 = pd.merge(neurobat1,neurobat_boston_naming_test , left_index = True, right_index = True, how='inner')\n",
    "neurobat1 = neurobat1.T\n",
    "neurobat1 = neurobat1[neurobat1.index.get_level_values(1).isin(list_months_to_be_considered)].T\n",
    "# reducing index level\n",
    "neurobat1_ruf = neurobat1.T.reset_index()\n",
    "neurobat1_ruf.iloc[:,0]  = 'neurobat__' + neurobat1_ruf.iloc[:,0] +  '___' + neurobat1_ruf.iloc[:,1]\n",
    "neurobat1_ruf = neurobat1_ruf.set_index('level_0')\n",
    "neurobat1 = neurobat1_ruf.iloc[:,1:].T\n",
    "neurobat1_no_encoding = neurobat1\n",
    "\n",
    "neurobat = neurobat1\n",
    "\n",
    "neurobat.name = 'Neuropsychological Battery (All combined)'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10617, 1)\n"
     ]
    }
   ],
   "source": [
    "#MMSE\n",
    "#cols['mmse'] = ['RID', 'VISCODE2','MMSCORE','MMDATE','MMYEAR','MMMONTH','MMDAY','MMSEASON','MMHOSPIT',    'MMFLOOR','MMCITY','MMAREA','MMSTATE','MMBALL','MMFLAG','MMTREE','MMD','MML','MMR','MMO','MMW',    'MMBALLDL','MMFLAGDL','MMTREEDL','MMWATCH','MMPENCIL','MMREPEAT','MMHAND','MMFOLD','MMONFLR','MMREAD',    'MMWRITE','MMDRAW']\n",
    "cols['mmse'] = ['RID', 'VISCODE2','MMSCORE']\n",
    "\n",
    "mmse = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\MMSE.csv', usecols=cols['mmse'], index_col = ['RID', 'VISCODE2'])\n",
    "mmse1 = mmse.copy(deep = True)\n",
    "print(mmse1.shape)\n",
    "mmse = mmse[~mmse.index.duplicated()].reset_index()\n",
    "mmse = mmse[mmse.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "mmse = mmse.replace({-4:np.NAN})\n",
    "mmse = mmse.replace({-1:np.NAN})\n",
    "mmse = mmse[ (mmse.isnull().sum(axis = 1) < 10) ]\n",
    "mmse = mmse.unstack()\n",
    "mmse = mmse[ (mmse.isnull().sum(axis = 1) < 20) ]\n",
    "mmse = mmse[~mmse.index.duplicated()]\n",
    "mmse = mmse.T\n",
    "mmse = mmse[mmse.index.get_level_values(1).isin(list_months_to_be_considered)].T\n",
    "\n",
    "# reducing index level\n",
    "mmse_ruf = mmse.T.reset_index()\n",
    "mmse_ruf.iloc[:,0]  = 'mmse__' + mmse_ruf.iloc[:,0] +  '___' + mmse_ruf.iloc[:,1]\n",
    "mmse_ruf = mmse_ruf.set_index('level_0')\n",
    "mmse = mmse_ruf.iloc[:,1:].T\n",
    "\n",
    "\n",
    "mmse_no_encoding = mmse\n",
    "## Hot encoding\n",
    "#mmse_name_list = list( mmse.columns )\n",
    "#mmse_empty = pd.DataFrame()\n",
    "#for i in range(len(mmse_name_list)):\n",
    "#    name = mmse_name_list[i]\n",
    "#    mmse_with_dummies = pd.get_dummies(mmse[name], sparse=True, drop_first=True, prefix=name)\n",
    "#    mmse_empty = pd.concat([mmse_empty,mmse_with_dummies] , axis = 1)   \n",
    "mmse = mmse_no_encoding    \n",
    "mmse.name = 'Mini Mental State Exam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9486, 2)\n"
     ]
    }
   ],
   "source": [
    "#GERIATRIC\n",
    "cols['geriatric'] = ['VISCODE2', 'RID', 'GDTOTAL']\n",
    "geriatric = pd.read_csv(\"ADNI\\\\Raw_Data\\\\Assessment\\\\GDSCALE.csv\", index_col='RID', usecols=cols['geriatric'])\n",
    "geriatric1 = geriatric.copy(deep = True)\n",
    "print(geriatric1.shape)\n",
    "geriatric = geriatric.replace({-4:np.NAN})\n",
    "geriatric = geriatric.replace({-1:np.NAN})\n",
    "geriatric = geriatric[geriatric['VISCODE2'].isin(['bl','m12','m06']) ]  \n",
    "geriatric = geriatric.reset_index().set_index(['RID','VISCODE2'])\n",
    "geriatric = geriatric[ (geriatric.isnull().sum(axis = 1) ==0) ]\n",
    "geriatric = geriatric[~geriatric.index.duplicated()].unstack()\n",
    "geriatric = geriatric[ (geriatric.isnull().sum(axis = 1) ==0) ]\n",
    "geriatric = geriatric.T\n",
    "geriatric = geriatric[geriatric.index.get_level_values(1).isin(list_months_to_be_considered)].T\n",
    "\n",
    "\n",
    "# reducing index level\n",
    "geriatric_ruf = geriatric.T.reset_index()\n",
    "geriatric_ruf.iloc[:,0]  = 'gd_scale__' + geriatric_ruf.iloc[:,0] +  '___' + geriatric_ruf.iloc[:,1]\n",
    "geriatric_ruf = geriatric_ruf.set_index('level_0')\n",
    "geriatric = geriatric_ruf.iloc[:,1:].T\n",
    "geriatric_no_encoding = geriatric\n",
    "\n",
    "geriatric.name = 'Geriatric depression scale'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9207, 2)\n"
     ]
    }
   ],
   "source": [
    "#UWNPSYCHSUM_10_27_17\n",
    "cols['UWNPSYCHSUM_10_27_17'] = ['RID', 'VISCODE2', 'ADNI_MEM', 'ADNI_EF']\n",
    "uwn = pd.DataFrame(pd.read_csv(\"ADNI\\\\Raw_Data\\\\Assessment\\\\UWNPSYCHSUM_10_27_17.csv\",index_col= ['RID','VISCODE2'], usecols=cols['UWNPSYCHSUM_10_27_17']))\n",
    "uwn1 = uwn.copy(deep = True)\n",
    "print(uwn1.shape)\n",
    "uwn = uwn[~uwn.index.duplicated()].reset_index()\n",
    "uwn = uwn[uwn.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "uwn = uwn[~uwn.index.duplicated()].unstack()\n",
    "uwn = uwn[ (uwn.isnull().sum(axis = 1) < 3) ]\n",
    "new_col_list_uwn = uwn.columns.levels[0]\n",
    "for a in new_col_list_uwn: \n",
    "    uwn[a] = uwn[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "uwn = uwn[ (uwn.isnull().sum(axis = 1) <1) ]\n",
    "uwn = uwn.T\n",
    "uwn = uwn[uwn.index.get_level_values(1).isin(list_months_to_be_considered)].T\n",
    "\n",
    "\n",
    "\n",
    "# reducing index level\n",
    "uwn_ruf = uwn.T.reset_index()\n",
    "uwn_ruf.iloc[:,0]  = 'UW__' + uwn_ruf.iloc[:,0] +  '___' + uwn_ruf.iloc[:,1]\n",
    "uwn_ruf = uwn_ruf.set_index('level_0')\n",
    "uwn = uwn_ruf.iloc[:,1:].T\n",
    "uwn_no_encoding = uwn\n",
    "uwn.name = 'Crane Lab (UW) Neuropsych Summary Score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4447, 12)\n"
     ]
    }
   ],
   "source": [
    "#NPI\n",
    "cols['npi'] = ['RID', 'VISCODE2','NPIATOT', 'NPIBTOT', 'NPICTOT',  'NPIDTOT',     'NPIETOT', 'NPIFTOT', 'NPIGTOT', 'NPIHTOT', 'NPIITOT', 'NPIJTOT','NPIKTOT', 'NPILTOT']\n",
    "npi = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\NPI.csv', usecols=cols['npi'], index_col = ['RID', 'VISCODE2'])\n",
    "npi1 = npi.copy(deep = True)\n",
    "print(npi1.shape)\n",
    "npi = npi[~npi.index.duplicated()].reset_index()\n",
    "# m12 only\n",
    "npi_m12 = npi[npi.VISCODE2.isin(['m12'])].set_index(['RID','VISCODE2'])\n",
    "npi_m12 = npi_m12[~npi_m12.index.duplicated()].unstack()\n",
    "npi_m12 = npi_m12[ (npi_m12.isnull().sum(axis = 1) < 12) ]\n",
    "# baseline\n",
    "npi_bl = npi[npi.VISCODE2.isin(['bl'])].set_index(['RID','VISCODE2'])\n",
    "npi_bl = npi_bl[~npi_bl.index.duplicated()].unstack()\n",
    "npi_bl = npi_bl[ (npi_bl.isnull().sum(axis = 1) < 12) ]\n",
    "# both baseline and m12\n",
    "npi_all = npi[npi.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "npi_all = npi_all[~npi_all.index.duplicated()].unstack()\n",
    "npi_all = npi_all[ (npi_all.isnull().sum(axis = 1) < 10) ]\n",
    "npi_all = npi_all.T\n",
    "npi_all = npi_all[npi_all.index.get_level_values(1).isin(list_months_to_be_considered)].T\n",
    "\n",
    "# reducing index level\n",
    "npi_all_ruf = npi_all.T.reset_index()\n",
    "npi_all_ruf.iloc[:,0]  = 'npi_all__' + npi_all_ruf.iloc[:,0] +  '___' + npi_all_ruf.iloc[:,1]\n",
    "npi_all_ruf = npi_all_ruf.set_index('level_0')\n",
    "npi_all = npi_all_ruf.iloc[:,1:].T\n",
    "npi_all_no_encoding = npi_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5509, 10)\n"
     ]
    }
   ],
   "source": [
    "#MOCA \n",
    "cols['moca'] = ['RID','VISCODE2', 'TRAILS', 'CUBE', 'CLOCKCON', 'CLOCKNO', 'CLOCKHAN','LION', 'RHINO', 'CAMEL', 'IMMT1W1', 'IMMT1W2', 'IMMT1W3', 'IMMT1W4',\n",
    "       'IMMT1W5', 'IMMT2W1', 'IMMT2W2', 'IMMT2W3', 'IMMT2W4', 'IMMT2W5','DIGFOR', 'DIGBACK', 'LETTERS', 'SERIAL1', 'SERIAL2', 'SERIAL3',\n",
    "       'SERIAL4', 'SERIAL5', 'REPEAT1', 'REPEAT2', 'FFLUENCY', 'ABSTRAN','ABSMEAS', 'DELW1', 'DELW2', 'DELW3', 'DELW4', 'DELW5', 'DATE', 'MONTH',\n",
    "       'YEAR', 'DAY', 'PLACE', 'CITY']\n",
    "\n",
    "cols['moca_trail_making'] = ['TRAILS']\n",
    "cols['moca_visuosoconstructional'] = ['CUBE', 'CLOCKCON', 'CLOCKNO', 'CLOCKHAN']\n",
    "cols['moca_naming'] = [ 'LION', 'RHINO', 'CAMEL']\n",
    "cols['moca_immediate_recall'] = [ 'IMMT2W1', 'IMMT2W2', 'IMMT2W3', 'IMMT2W4', 'IMMT2W5','IMMT1W1', 'IMMT1W2', 'IMMT1W3', 'IMMT1W4', 'IMMT1W5']\n",
    "cols['moca_attention'] = [ 'DIGFOR', 'DIGBACK', 'LETTERS', 'SERIAL1', 'SERIAL2', 'SERIAL3','SERIAL4', 'SERIAL5']\n",
    "cols['moca_sen_repetetion'] = ['REPEAT1','REPEAT2']\n",
    "cols['moca_fluency'] = ['FFLUENCY']\n",
    "cols['moca_abstraction'] = ['ABSTRAN','ABSMEAS']\n",
    "cols['moca_delayed_word_recall'] = [ 'DELW1', 'DELW2', 'DELW3', 'DELW4', 'DELW5']\n",
    "cols['moca_orientation'] = [ 'DATE', 'MONTH', 'YEAR', 'DAY', 'PLACE', 'CITY' ]\n",
    "moca = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\MOCA.csv', usecols=cols['moca'], index_col = ['RID', 'VISCODE2'])\n",
    "moca['moca_trail_making'] = moca[cols['moca_trail_making']].sum(axis=1)\n",
    "moca['moca_visuosoconstructional'] = moca[cols['moca_visuosoconstructional']].sum(axis=1)\n",
    "moca['moca_naming'] = moca[cols['moca_naming']].sum(axis = 1)\n",
    "moca['moca_immediate_recall'] = moca[cols['moca_immediate_recall']].sum(axis=1)\n",
    "moca['moca_attention'] = moca[cols['moca_attention']].sum(axis=1)\n",
    "moca['moca_sen_repetetion'] = moca[cols['moca_sen_repetetion']].sum(axis=1)\n",
    "moca['moca_fluency'] = moca[cols['moca_fluency']].sum(axis=1)\n",
    "moca['moca_abstraction'] = moca[cols['moca_abstraction']].sum(axis=1)\n",
    "moca['moca_delayed_word_recall'] = moca[cols['moca_delayed_word_recall']].sum(axis=1)\n",
    "moca['moca_orientation'] = moca[cols['moca_orientation']].sum(axis=1)\n",
    "moca = moca[['moca_trail_making', 'moca_visuosoconstructional', 'moca_naming', 'moca_attention', 'moca_immediate_recall',             'moca_sen_repetetion', 'moca_fluency','moca_abstraction','moca_delayed_word_recall','moca_orientation']] # drop extra\n",
    "moca1 = moca.copy(deep = True) \n",
    "print(moca1.shape)\n",
    "#Dropping the Duplicated Index (Only 1)\n",
    "moca = moca[~moca.index.duplicated()].reset_index()\n",
    "moca = moca[moca.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "moca = moca.unstack()\n",
    "moca = moca[ (moca.isnull().sum(axis = 1) < 15) ]\n",
    "new_col_list_moca = moca.columns.levels[0]\n",
    "for a in new_col_list_moca: \n",
    "    moca[a] = moca[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "moca = moca.T\n",
    "moca = moca[moca.index.get_level_values(1).isin(list_months_to_be_considered)].T\n",
    "  \n",
    "    \n",
    "# reducing index level\n",
    "moca_ruf = moca.T.reset_index()\n",
    "moca_ruf.iloc[:,0]  = 'moca__' + moca_ruf.iloc[:,0] +  '___' + moca_ruf.iloc[:,1]\n",
    "moca_ruf = moca_ruf.set_index('level_0')\n",
    "moca = moca_ruf.iloc[:,1:].T\n",
    "moca_no_encoding = moca\n",
    "\n",
    "\n",
    "## Hot encoding\n",
    "#moca_name_list = list( moca.columns )\n",
    "#moca_empty = pd.DataFrame()\n",
    "#for i in range(len(moca_name_list)):\n",
    "#    name = moca_name_list[i]\n",
    "#    moca_with_dummies = pd.get_dummies(moca[name], sparse=True, drop_first=True, prefix=name)\n",
    "#    indexex = moca_with_dummies.index \n",
    "#    moca_empty = pd.concat([moca_empty,moca_with_dummies] , axis = 1)  \n",
    "#moca = moca_empty        \n",
    "\n",
    "moca = moca_no_encoding\n",
    "moca.name = 'Montreal Cognitive Assessment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9525, 1)\n"
     ]
    }
   ],
   "source": [
    "#FAQ\n",
    "cols['faq'] = ['RID', 'VISCODE2', 'FAQTOTAL']\n",
    "faq = pd.read_csv('ADNI\\\\Raw_Data\\\\Assessment\\\\FAQ.csv', usecols=cols['faq'], index_col = ['RID', 'VISCODE2'])\n",
    "faq1 = faq.copy(deep = True) \n",
    "print(faq1.shape)\n",
    "faq = faq[~faq.index.duplicated()].reset_index()\n",
    "faq = faq[faq.VISCODE2.isin(['bl','m06','m12'])].set_index(['RID','VISCODE2'])\n",
    "faq = faq[~faq.index.duplicated()]\n",
    "#Unstacking \n",
    "faq = faq.unstack()\n",
    "faq = faq[ (faq.isnull().sum(axis = 1) < 2) ]\n",
    "new_col_list_faq = faq.columns.levels[0]\n",
    "for a in new_col_list_faq: \n",
    "    faq[a] = faq[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "    \n",
    "faq = faq.T\n",
    "faq = faq[faq.index.get_level_values(1).isin(list_months_to_be_considered)].T\n",
    "      \n",
    "# reducing index level\n",
    "faq_ruf = faq.T.reset_index()\n",
    "faq_ruf.iloc[:,0]  = 'FAQ__' + faq_ruf.iloc[:,0] +  '___' + faq_ruf.iloc[:,1]\n",
    "faq_ruf = faq_ruf.set_index('level_0')\n",
    "faq = faq_ruf.iloc[:,1:].T\n",
    "faq_no_encoding = faq\n",
    "\n",
    "## Hot encoding\n",
    "#faq_name_list = list( faq.columns )\n",
    "#faq_empty = pd.DataFrame()\n",
    "#for i in range(len(faq_name_list)):\n",
    "#    name = faq_name_list[i]\n",
    "#    faq_with_dummies = pd.get_dummies(faq[name], sparse=True, drop_first=True, prefix=name)\n",
    "#    faq_empty = pd.concat([faq_empty,faq_with_dummies] , axis = 1)\n",
    "#faq = faq_empty    \n",
    "faq = faq_no_encoding\n",
    "faq.name = 'Functional Assessment Questionnaire'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5238, 40)\n"
     ]
    }
   ],
   "source": [
    "#ECOGPT\n",
    "cols['ecogpt'] =  ['RID', 'VISCODE2',        'MEMORY1', 'MEMORY2', 'MEMORY3', 'MEMORY4', 'MEMORY5',       'MEMORY6', 'MEMORY7', 'MEMORY8', 'LANG1', 'LANG2', 'LANG3', 'LANG4',       'LANG5', 'LANG6', 'LANG7', 'LANG8', 'LANG9', 'VISSPAT1', 'VISSPAT2',       'VISSPAT3', 'VISSPAT4', 'VISSPAT6', 'VISSPAT7', 'VISSPAT8',       'PLAN1', 'PLAN2', 'PLAN3', 'PLAN4', 'PLAN5', 'ORGAN1', 'ORGAN2',       'ORGAN3', 'ORGAN4', 'ORGAN5', 'ORGAN6', 'DIVATT1', 'DIVATT2', 'DIVATT3',       'DIVATT4']\n",
    "ecogpt = pd.read_csv(\"ADNI\\\\Raw_Data\\\\Assessment\\\\ecogpt.csv\",index_col='RID', usecols=cols['ecogpt'])\n",
    "\n",
    "list_memory = [ 'MEMORY1', 'MEMORY2', 'MEMORY3', 'MEMORY4', 'MEMORY5',       'MEMORY6', 'MEMORY7', 'MEMORY8']\n",
    "list_lang = ['LANG1', 'LANG2', 'LANG3', 'LANG4',       'LANG5', 'LANG6', 'LANG7', 'LANG8', 'LANG9']\n",
    "list_vis = ['VISSPAT1', 'VISSPAT2',       'VISSPAT3', 'VISSPAT4', 'VISSPAT6', 'VISSPAT7', 'VISSPAT8']\n",
    "list_plan = ['PLAN1', 'PLAN2', 'PLAN3', 'PLAN4', 'PLAN5']\n",
    "list_org = ['ORGAN1', 'ORGAN2',       'ORGAN3', 'ORGAN4', 'ORGAN5', 'ORGAN6']\n",
    "list_div = ['DIVATT1', 'DIVATT2', 'DIVATT3',       'DIVATT4']\n",
    "\n",
    "ecogpt_new = pd.DataFrame()\n",
    "ecogpt1 = ecogpt.copy(deep = True)\n",
    "print(ecogpt1.shape)\n",
    "ecogpt = ecogpt[ecogpt['VISCODE2'].isin(['bl','m12','m06']) ]  \n",
    "ecogpt = ecogpt.reset_index().set_index(['RID','VISCODE2'])\n",
    "ecogpt = ecogpt[~ecogpt.index.duplicated()]\n",
    "ecogpt = ecogpt[ (ecogpt.isnull().sum(axis = 1) <= 30) ]\n",
    "ecogpt = ecogpt.unstack()\n",
    "ecogpt = ecogpt[ (ecogpt.isnull().sum(axis = 1) < 41) ]\n",
    "new_col_list_ecogpt = ecogpt.columns.levels[0]\n",
    "for a in new_col_list_ecogpt: \n",
    "    ecogpt[a] = ecogpt[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "    \n",
    "ecogpt = ecogpt.replace({9: None })\n",
    "ecogpt = ecogpt.T\n",
    "ecogpt_bl = ecogpt[ecogpt.index.get_level_values(1).isin(['bl'])]\n",
    "ecogpt_m06 = ecogpt[ecogpt.index.get_level_values(1).isin(['m06'])]\n",
    "ecogpt_m12 = ecogpt[ecogpt.index.get_level_values(1).isin(['m12'])]\n",
    " \n",
    "ecogpt_new['memory___bl']  =  ecogpt_bl[ecogpt_bl.index.get_level_values(0).isin(list_memory)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['lang___bl']  =  ecogpt_bl[ecogpt_bl.index.get_level_values(0).isin(list_lang)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['vis___bl']  =  ecogpt_bl[ecogpt_bl.index.get_level_values(0).isin(list_vis)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['plan___bl']  =  ecogpt_bl[ecogpt_bl.index.get_level_values(0).isin(list_plan)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['org___bl']  =  ecogpt_bl[ecogpt_bl.index.get_level_values(0).isin(list_org)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['division___bl']  =  ecogpt_bl[ecogpt_bl.index.get_level_values(0).isin(list_div)].mean( axis = 0, skipna = True  )\n",
    "\n",
    "ecogpt_new['memory___m06']  =  ecogpt_m06[ecogpt_m06.index.get_level_values(0).isin(list_memory)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['lang___m06']  =  ecogpt_m06[ecogpt_m06.index.get_level_values(0).isin(list_lang)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['vis___m06']  =  ecogpt_m06[ecogpt_m06.index.get_level_values(0).isin(list_vis)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['plan___m06']  =  ecogpt_m06[ecogpt_m06.index.get_level_values(0).isin(list_plan)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['org___m06']  =  ecogpt_m06[ecogpt_m06.index.get_level_values(0).isin(list_org)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['division___m06']  =  ecogpt_m06[ecogpt_m06.index.get_level_values(0).isin(list_div)].mean( axis = 0, skipna = True  )\n",
    "\n",
    "ecogpt_new['memory___m12']  =  ecogpt_m12[ecogpt_m12.index.get_level_values(0).isin(list_memory)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['lang___m12']  =  ecogpt_m12[ecogpt_m12.index.get_level_values(0).isin(list_lang)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['vis___m12']  =  ecogpt_m12[ecogpt_m12.index.get_level_values(0).isin(list_vis)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['plan___m12']  =  ecogpt_m12[ecogpt_m12.index.get_level_values(0).isin(list_plan)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['org___m12']  =  ecogpt_m12[ecogpt_m12.index.get_level_values(0).isin(list_org)].mean( axis = 0, skipna = True  )\n",
    "ecogpt_new['division___m12']  =  ecogpt_m12[ecogpt_m12.index.get_level_values(0).isin(list_div)].mean( axis = 0, skipna = True  )\n",
    "\n",
    "\n",
    "\n",
    "# reducing index level\n",
    "ecogpt_ruf = ecogpt_new.T.reset_index()\n",
    "ecogpt_ruf.iloc[:,0]  = 'ecogpt_'  + ecogpt_ruf.iloc[:,0]\n",
    "ecogpt_ruf = ecogpt_ruf.set_index(['index'])\n",
    "ecogpt_ruf = ecogpt_ruf.T\n",
    "ecogpt_no_encoding = ecogpt_ruf\n",
    "\n",
    "\n",
    "## Hot encoding\n",
    "#ecogpt_name_list = list( ecogpt.columns )\n",
    "#ecogpt_empty = pd.DataFrame()\n",
    "#for i in range(len(ecogpt_name_list)):\n",
    "#    name = ecogpt_name_list[i]\n",
    "#    ecogpt_with_dummies = pd.get_dummies(ecogpt[name], sparse=True, drop_first=True, prefix=name)\n",
    "#    ecogpt_empty = pd.concat([ecogpt_empty,ecogpt_with_dummies] , axis = 1)\n",
    "#    \n",
    "ecogpt = ecogpt_no_encoding   \n",
    "ecogpt.name = 'Everyday cognition - study partner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5463, 40)\n"
     ]
    }
   ],
   "source": [
    "#EXOGSP\n",
    "cols['ecogsp'] =  ['RID', 'VISCODE2',        'MEMORY1', 'MEMORY2', 'MEMORY3', 'MEMORY4', 'MEMORY5',       'MEMORY6', 'MEMORY7', 'MEMORY8', 'LANG1', 'LANG2', 'LANG3', 'LANG4',       'LANG5', 'LANG6', 'LANG7', 'LANG8', 'LANG9', 'VISSPAT1', 'VISSPAT2',       'VISSPAT3', 'VISSPAT4', 'VISSPAT6', 'VISSPAT7', 'VISSPAT8',       'PLAN1', 'PLAN2', 'PLAN3', 'PLAN4', 'PLAN5', 'ORGAN1', 'ORGAN2',       'ORGAN3', 'ORGAN4', 'ORGAN5', 'ORGAN6', 'DIVATT1', 'DIVATT2', 'DIVATT3',       'DIVATT4']\n",
    "ecogsp = pd.read_csv(\"ADNI\\\\Raw_Data\\\\Assessment\\\\ECOGSP.csv\",index_col='RID', usecols=cols['ecogsp'])\n",
    "\n",
    "list_memory = [ 'MEMORY1', 'MEMORY2', 'MEMORY3', 'MEMORY4', 'MEMORY5',       'MEMORY6', 'MEMORY7', 'MEMORY8']\n",
    "list_lang = ['LANG1', 'LANG2', 'LANG3', 'LANG4',       'LANG5', 'LANG6', 'LANG7', 'LANG8', 'LANG9']\n",
    "list_vis = ['VISSPAT1', 'VISSPAT2',       'VISSPAT3', 'VISSPAT4', 'VISSPAT6', 'VISSPAT7', 'VISSPAT8']\n",
    "list_plan = ['PLAN1', 'PLAN2', 'PLAN3', 'PLAN4', 'PLAN5']\n",
    "list_org = ['ORGAN1', 'ORGAN2',       'ORGAN3', 'ORGAN4', 'ORGAN5', 'ORGAN6']\n",
    "list_div = ['DIVATT1', 'DIVATT2', 'DIVATT3',       'DIVATT4']\n",
    "\n",
    "ecogsp_new = pd.DataFrame()\n",
    "ecogsp1 = ecogsp.copy(deep = True)\n",
    "print(ecogsp1.shape)\n",
    "ecogsp = ecogsp[ecogsp['VISCODE2'].isin(['bl','m12','m06']) ]  \n",
    "ecogsp = ecogsp.reset_index().set_index(['RID','VISCODE2'])\n",
    "ecogsp = ecogsp[~ecogsp.index.duplicated()]\n",
    "ecogsp = ecogsp[ (ecogsp.isnull().sum(axis = 1) <= 30) ]\n",
    "ecogsp = ecogsp.unstack()\n",
    "ecogsp = ecogsp[ (ecogsp.isnull().sum(axis = 1) < 41) ]\n",
    "new_col_list_ecogsp = ecogsp.columns.levels[0]\n",
    "for a in new_col_list_ecogsp: \n",
    "    ecogsp[a] = ecogsp[a].interpolate(method='linear', axis=1, limit=1, limit_direction='both')\n",
    "    \n",
    "ecogsp = ecogsp.replace({9: None })\n",
    "ecogsp = ecogsp.T\n",
    "ecogsp_bl = ecogsp[ecogsp.index.get_level_values(1).isin(['bl'])]\n",
    "ecogsp_m06 = ecogsp[ecogsp.index.get_level_values(1).isin(['m06'])]\n",
    "ecogsp_m12 = ecogsp[ecogsp.index.get_level_values(1).isin(['m12'])]\n",
    " \n",
    "ecogsp_new['memory___bl']  =  ecogsp_bl[ecogsp_bl.index.get_level_values(0).isin(list_memory)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['lang___bl']  =  ecogsp_bl[ecogsp_bl.index.get_level_values(0).isin(list_lang)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['vis___bl']  =  ecogsp_bl[ecogsp_bl.index.get_level_values(0).isin(list_vis)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['plan___bl']  =  ecogsp_bl[ecogsp_bl.index.get_level_values(0).isin(list_plan)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['org___bl']  =  ecogsp_bl[ecogsp_bl.index.get_level_values(0).isin(list_org)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['division___bl']  =  ecogsp_bl[ecogsp_bl.index.get_level_values(0).isin(list_div)].mean( axis = 0, skipna = True  )\n",
    "\n",
    "ecogsp_new['memory___m06']  =  ecogsp_m06[ecogsp_m06.index.get_level_values(0).isin(list_memory)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['lang___m06']  =  ecogsp_m06[ecogsp_m06.index.get_level_values(0).isin(list_lang)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['vis___m06']  =  ecogsp_m06[ecogsp_m06.index.get_level_values(0).isin(list_vis)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['plan___m06']  =  ecogsp_m06[ecogsp_m06.index.get_level_values(0).isin(list_plan)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['org___m06']  =  ecogsp_m06[ecogsp_m06.index.get_level_values(0).isin(list_org)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['division___m06']  =  ecogsp_m06[ecogsp_m06.index.get_level_values(0).isin(list_div)].mean( axis = 0, skipna = True  )\n",
    "\n",
    "ecogsp_new['memory___m12']  =  ecogsp_m12[ecogsp_m12.index.get_level_values(0).isin(list_memory)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['lang___m12']  =  ecogsp_m12[ecogsp_m12.index.get_level_values(0).isin(list_lang)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['vis___m12']  =  ecogsp_m12[ecogsp_m12.index.get_level_values(0).isin(list_vis)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['plan___m12']  =  ecogsp_m12[ecogsp_m12.index.get_level_values(0).isin(list_plan)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['org___m12']  =  ecogsp_m12[ecogsp_m12.index.get_level_values(0).isin(list_org)].mean( axis = 0, skipna = True  )\n",
    "ecogsp_new['division___m12']  =  ecogsp_m12[ecogsp_m12.index.get_level_values(0).isin(list_div)].mean( axis = 0, skipna = True  )\n",
    "ecogsp = ecogsp.dropna(axis=0)\n",
    "# reducing index level\n",
    "ecogsp_ruf = ecogsp_new.T.reset_index()\n",
    "ecogsp_ruf.iloc[:,0]  = 'ecogsp_'  + ecogsp_ruf.iloc[:,0]\n",
    "ecogsp_ruf = ecogsp_ruf.set_index(['index'])\n",
    "ecogsp_ruf = ecogsp_ruf.T\n",
    "ecogsp_no_encoding = ecogsp_ruf\n",
    "\n",
    "## Hot encoding\n",
    "#ecogsp_name_list = list( ecogsp.columns )\n",
    "#ecogsp_empty = pd.DataFrame()\n",
    "#for i in range(len(ecogsp_name_list)):\n",
    "#    name = ecogsp_name_list[i]\n",
    "#    ecogsp_with_dummies = pd.get_dummies(ecogsp[name], sparse=True, drop_first=True, prefix=name)\n",
    "#    ecogsp_empty = pd.concat([ecogsp_empty,ecogsp_with_dummies] , axis = 1)\n",
    "#    \n",
    "ecogsp = ecogsp_no_encoding   \n",
    "ecogsp = ecogsp.dropna(axis=0)\n",
    "ecogsp.name = 'Everyday cognition - self'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************\n",
      "Datasets considered ['moca', 'neurobat', 'npi_all', 'mmse', 'geriatric', 'ecogsp', 'ecogpt', 'cdr', 'faq'] :\n",
      "************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# datasets and visits of interest\n",
    "# dict_datasets dictionary will contain all the data sets.\n",
    "# common_rids is a list of all the common ridsin the datset\n",
    "datasets_of_interest = ['moca', 'neurobat','npi_all', 'mmse', 'geriatric', 'ecogsp', 'ecogpt', 'cdr' , 'faq' ]\n",
    "dict_datasets = {}\n",
    "dict_datasets['moca'] = moca    \n",
    "dict_datasets['neurobat'] = neurobat    \n",
    "dict_datasets['npi_all'] = npi_all\n",
    "dict_datasets['mmse'] = mmse             \n",
    "dict_datasets['geriatric'] = geriatric   \n",
    "dict_datasets['ecogsp'] = ecogsp\n",
    "#dict_datasets['UWNPSYCHSUM_10_27_17'] = uwn\n",
    "dict_datasets['ecogpt'] = ecogpt\n",
    "dict_datasets['cdr'] = cdr\n",
    "dict_datasets['faq'] = faq \n",
    "\n",
    "\n",
    "size_matrix = pd.DataFrame(np.zeros((len(dict_datasets),2)) )\n",
    "for r in range(len(dict_datasets) ):\n",
    "    size_matrix.iloc[r,0] = datasets_of_interest[r]\n",
    "    size_matrix.iloc[r,1] = len(dict_datasets[datasets_of_interest[r]])\n",
    "\n",
    "size_matrix.columns = ['dataset','count']\n",
    "size_matrix =size_matrix.set_index('dataset')\n",
    "size_matrix = size_matrix.sort_values(by = ['count'],ascending= False )\n",
    "             \n",
    "sorted_cols = list(size_matrix.index)\n",
    "common_rids = pd.DataFrame(np.zeros((len(dict_datasets),len(dict_datasets))))\n",
    "common_rids.columns = sorted_cols\n",
    "common_rids.index = sorted_cols\n",
    "\n",
    "for i in range(len(dict_datasets)):\n",
    "    for u in range(len(dict_datasets)):\n",
    "        if (u>=i):\n",
    "            a = list(dict_datasets[sorted_cols[i]].index)\n",
    "            b = list(dict_datasets[sorted_cols[u]].index)\n",
    "        \n",
    "            common = list(set(a).intersection(b))\n",
    "            common_rids.iloc[i,u] = len(common)\n",
    "\n",
    "Max_intersection_dataset_item = ['moca', 'neurobat','npi_all', 'mmse', 'geriatric', 'ecogsp', 'ecogpt', 'cdr' , 'faq' ]\n",
    "\n",
    "print('************************')\n",
    "print('Datasets considered {} :'.format(Max_intersection_dataset_item)  )\n",
    "print('************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_data(Max_intersection_dataset, visit, list_months_to_be_considered):\n",
    "    patno_filtered_visited = dict_datasets[Max_intersection_dataset[0]]\n",
    "\n",
    "    for t in range(len(Max_intersection_dataset)-1):\n",
    "        patients = dict_datasets[Max_intersection_dataset[t+1]]\n",
    "        patno_filtered_visited = pd.merge(patno_filtered_visited, patients, left_index = True, right_index = True, how='inner')\n",
    "        \n",
    "    M_chosen = normalize(patno_filtered_visited,'m')\n",
    "    M_chosen = M_chosen.T[ M_chosen.T.isnull().sum(axis = 1)== 0 ].T\n",
    "    print(M_chosen.shape)\n",
    "    M_W_columns = ['PCA_1', 'PCA_2', 'PCA_3', 'PCA_2_1', 'PCA_2_2','ICA_1', 'ICA_2', 'NMF_2_1', 'NMF_2_2', \n",
    "               'NMF_3_1', 'NMF_3_2', 'NMF_3_3','ICA_3_1', 'ICA_3_2', 'ICA_3_3']\n",
    "    M_W = pd.DataFrame(index=M_chosen.index, columns=M_W_columns)\n",
    "    \n",
    "    # PCA\n",
    "    model_pca = sklearnPCA(n_components=3)\n",
    "    M_W[['PCA_1', 'PCA_2', 'PCA_3']] = model_pca.fit_transform(M_chosen)\n",
    "    model_pca = sklearnPCA(n_components=2)\n",
    "    M_W[['PCA_2_1', 'PCA_2_2']] = model_pca.fit_transform(M_chosen)\n",
    "\n",
    "    # ICA\n",
    "    model_ICA = decomposition.FastICA(n_components=2)\n",
    "    M_W[['ICA_1', 'ICA_2']] = model_ICA.fit_transform(M_chosen)\n",
    "    model_ICA = decomposition.FastICA(n_components=3)\n",
    "    M_W[['ICA_3_1', 'ICA_3_2', 'ICA_3_3']] = model_ICA.fit_transform(M_chosen)\n",
    "\n",
    "    # NMF\n",
    "    model_NMF = decomposition.NMF(n_components=2, init='nndsvda', max_iter=200)\n",
    "    model_NMF3 = decomposition.NMF(n_components=3, init='nndsvda', max_iter=200)\n",
    "    M_W[['NMF_2_1', 'NMF_2_2']] = model_NMF.fit_transform(M_chosen)\n",
    "    M_W[['NMF_3_1', 'NMF_3_2', 'NMF_3_3']] = model_NMF3.fit_transform(M_chosen)\n",
    "    \n",
    "    H = model_NMF.components_\n",
    "    H_columns = M_chosen.columns\n",
    "    M_H = pd.DataFrame(columns=H_columns)\n",
    "    M_H.loc[0] = H[0,:]\n",
    "    M_H.loc[1] = H[1,:]\n",
    "    M_H_T = M_H.T.sort_values(by=[1],ascending=False)\n",
    "    M_H_T3 = M_H_T\n",
    "    M_H_T.columns = ['axis 1','axis 2']\n",
    "    M_H_T = pd.DataFrame(M_H_T)\n",
    "    M_H_T = M_H_T.div(M_H_T.sum(axis=1), axis=0)\n",
    "    M_H_T['new'] = 0\n",
    "    M_H_T['new'] = M_H_T.apply(lambda M_H_T :  'axis 1' if (M_H_T['axis 1']> M_H_T['axis 2']+(M_H_T['axis 2'] *0.5) ) else 'axis 2' if (M_H_T['axis 2'] > M_H_T['axis 1'] +(M_H_T['axis 1']*0.5) ) else 'ambigious', axis=1)\n",
    "    M_H_T2 = M_H_T\n",
    "    M_H_T.to_csv(address + \"all_2d_list.csv\") \n",
    "    \n",
    "    H = model_NMF3.components_\n",
    "    H_columns = M_chosen.columns\n",
    "    M_H = pd.DataFrame(columns=H_columns)\n",
    "    M_H.loc[0] = H[0,:]\n",
    "    M_H.loc[1] = H[1,:]\n",
    "    M_H.loc[2] = H[2,:]\n",
    "    M_H_T = M_H.T.sort_values(by=[2],ascending=False)\n",
    "    M_H_T.columns = ['axis 1','axis 2', 'axis 3']\n",
    "    M_H_T = pd.DataFrame(M_H_T)\n",
    "    M_H_T = M_H_T.div(M_H_T.sum(axis=1), axis=0)\n",
    "    M_H_T['new'] = 0\n",
    "    M_H_T['new'] = M_H_T.apply(lambda M_H_T :  'axis 1' if (M_H_T['axis 1']> M_H_T['axis 2']+M_H_T['axis 3'] ) else 'axis 2' if (M_H_T['axis 2'] > M_H_T['axis 1'] +M_H_T['axis 3']) else 'axis 3' if (M_H_T['axis 3'] > M_H_T['axis 1'] +M_H_T['axis 2']) else 'ambigious'  , axis=1)\n",
    "    M_H_T.to_csv(address + \"all_3d_list.csv\") \n",
    "    M_H_T3 = M_H_T\n",
    "    redued_data = pd.DataFrame(M_W) # this datset contains all the ICA, PCA and NMF vectors\n",
    "    # plot the dimension reduction color makrked with participants' \"categories\", and \"gender\"\n",
    "    dignosis = pd.read_csv(\"ADNI\\\\Raw_Data\\\\Assessment\\\\dxsum.csv\",  usecols= ['RID','DXCHANGE','DXMDUE','DXCONFID','VISCODE'])\n",
    "    dignosis = dignosis[ ~(dignosis['DXCHANGE'].isnull())]\n",
    "    dignosis = dignosis[ ~(dignosis['DXMDUE'] == 'MCI due to other etiology')]\n",
    "    dignosis = dignosis[ ~(dignosis['DXCONFID'] == 'Mildly Confident')]\n",
    "    dignosis = dignosis[ ~(dignosis['DXCONFID'] == 'Uncertain')]\n",
    "    dignosis = dignosis[dignosis['RID'].isin(redued_data.index)]\n",
    "    dignosis = dignosis.set_index('RID')\n",
    "    dignosis = dignosis[dignosis['VISCODE'] == visit]\n",
    "    redued = redued_data.merge(dignosis, how = 'inner', left_index = True, right_index = True)\n",
    "    redued = redued[ ~(redued['DXCHANGE'].isnull())]   \n",
    "    redued.DXCHANGE = redued.DXCHANGE.replace(['Stable: NL to NL', 'Stable: NL','Stable: MCI','Stable: MCI to MCI',                                               'Stable: Dementia', 'Stable: Dementia to Dementia',                                               'Conversion: NL to MCI','Conversion: MCI to Dementia','Conversion: NL to Dementia',                                               'Reversion: MCI to NL', 'Reversion: Dementia to MCI'],[1,1,2,2,3,3,4,5,6,7,8])\n",
    "    # Replacing the codes as described earlier\n",
    "    redued.DXCHANGE = redued.DXCHANGE.replace([1,2,3,4,5,6,7,8],[1,2,3,2,3,3,1,2]) \n",
    "    colors_categories = redued.DXCHANGE.replace([1,2,3], ['red', 'blue', 'green'])\n",
    "    # use this or above 2 lines\n",
    "    return redued, colors_categories, M_chosen,dignosis,M_H_T2, M_H_T3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(572, 206)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "redued_item_24, colors_categories_item_24, M_chosen_item_24,dignosis,M_H_T2, M_H_T3 = project_data(Max_intersection_dataset_item, visits, list_months_to_be_considered)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    122\n",
      "2    104\n",
      "0     52\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "M_mci_dem = redued_item_24\n",
    "M_mci_dem_nmf_all = M_mci_dem[['NMF_2_1','NMF_2_2','NMF_3_1', 'NMF_3_2','NMF_3_3']].copy()\n",
    "M_mci_dem_nmf = M_mci_dem[['NMF_2_1', 'NMF_2_2']]\n",
    "M_mci_dem_nmf_proj_all = M_mci_dem_nmf_all[~(redued_item_24.DXCHANGE.isin([1]) )] # removing controls\n",
    "M_mci_dem_nmf_proj_3d_only = M_mci_dem_nmf_proj_all[['NMF_3_1','NMF_3_2','NMF_3_3']]\n",
    "M_mci_dem_nmf_proj = M_mci_dem_nmf_proj_all[['NMF_2_1','NMF_2_2']]\n",
    "\n",
    "try:\n",
    "    colors_categories_item_24_no_controls = redued_item_24[~(redued_item_24.DXCHANGE.isin([1]) )]['DXCHANGE'].replace([1,2,3], ['red', 'blue', 'green'])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def organize_prediction_moca(M_mci_dem_nmf_proj_3d_only,Predict_gmm):\n",
    "    M_mci_dem_nmf_proj = M_mci_dem_nmf_proj_3d_only\n",
    "    M_mci_dem_nmf_proj['predicted'] = Predict_gmm\n",
    "    a = list(pd.unique(Predict_gmm.iloc[:,0]))\n",
    "    srt = np.empty((len(a),2))\n",
    "    for i in a:\n",
    "        a = M_mci_dem_nmf_proj[M_mci_dem_nmf_proj.predicted == i].iloc[:,1].sum() / len(M_mci_dem_nmf_proj[M_mci_dem_nmf_proj.predicted == i])\n",
    "        #b =  M_mci_dem_nmf_proj[M_mci_dem_nmf_proj.predicted == i].iloc[:,1].sum() / len(M_mci_dem_nmf_proj[M_mci_dem_nmf_proj.predicted == i])\n",
    "        #c =  M_mci_dem_nmf_proj[M_mci_dem_nmf_proj.predicted == i].iloc[:,0].sum() / len(M_mci_dem_nmf_proj[M_mci_dem_nmf_proj.predicted == i])\n",
    "        srt[i,1] = a\n",
    "        srt[i,0] = i\n",
    "    srt = pd.DataFrame(srt).sort_values([1])\n",
    "    Predict_gmm.replace([srt.iloc[0,0],srt.iloc[1,0], srt.iloc[2,0] ],[0,1,2], inplace=True)   \n",
    "    return pd.DataFrame(Predict_gmm)\n",
    "\n",
    "\n",
    "from sklearn import mixture\n",
    "model_gmm = mixture.GaussianMixture(n_components=3, covariance_type='diag',  random_state = 0)\n",
    "model_gmm.fit(M_mci_dem_nmf_proj) # print(gmm.means_)\n",
    "# label the predicted and only keep HC and PDs\n",
    "Predict_gmm = pd.DataFrame(model_gmm.predict(M_mci_dem_nmf_proj))\n",
    "print(Predict_gmm.iloc[:,0].value_counts())\n",
    "Predict_gmm.columns = ['predicted']\n",
    "Predict_gmm.index = M_mci_dem_nmf_proj.index\n",
    "Predict_gmm = organize_prediction_moca(M_mci_dem_nmf_proj,Predict_gmm)\n",
    "M_mci_dem_nmf_proj['predicted'] = Predict_gmm\n",
    "#plot_side_by_side_2d(M_mci_dem_nmf_proj,Predict_gmm,redued_item_24,colors_categories_item_24,'item24','gmm')    \n",
    "nl_data = M_mci_dem_nmf[(redued_item_24.DXCHANGE.isin([1]) )]\n",
    "data_prediction_labels = pd.concat([nl_data,M_mci_dem_nmf_proj]).fillna(3)\n",
    "\n",
    "cols['apoe4'] = ['RID' , 'VISCODE' , 'APOE4'   ]\n",
    "apoe4 = pd.read_csv(\"C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\Raw_Data\\\\Assessment\\\\apoe4\\\\ADNIMERGE.csv\",index_col='RID', usecols=cols['apoe4'])\n",
    "apoe4['VISCODE2'] = apoe4['VISCODE']\n",
    "apoe4['VISCODE'].value_counts()\n",
    "del apoe4['VISCODE']\n",
    "apoe4 = apoe4[apoe4['VISCODE2'].isin([visits]) ] \n",
    "Predict_gmm = data_prediction_labels\n",
    "redued = Predict_gmm.merge(apoe4['APOE4'].to_frame(), left_index=True, right_index=True)\n",
    "redued = redued.merge(redued_item_24['DXCHANGE'].to_frame(), left_index=True, right_index=True)\n",
    "\n",
    "redued['predicted'] = redued['predicted'].replace([0,1,2,3],['Low','Moderate','High','Controls'])\n",
    "redued['DXCHANGE'] = redued['DXCHANGE'].replace([1,2,3],['Controls','MCI','Dementia'])\n",
    "\n",
    "\n",
    "redued_2 = redued[['NMF_2_1','predicted']]\n",
    "redued_2['Progression Vector'] = 1\n",
    "redued_2.columns = ['val','predicted','Progression Vector']\n",
    "redued_1 = redued[['NMF_2_2','predicted']]\n",
    "redued_1['Progression Vector'] = 2\n",
    "redued_1.columns = ['val','predicted','Progression Vector']\n",
    "redued_new = pd.concat([redued_1,redued_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'predicted' in list(M_chosen_item_24.columns):\n",
    "    print('Labels in train dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(446, 206)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NMF_2_1</th>\n",
       "      <th>NMF_2_2</th>\n",
       "      <th>predicted</th>\n",
       "      <th>APOE4</th>\n",
       "      <th>DXCHANGE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4003</th>\n",
       "      <td>0.497158</td>\n",
       "      <td>0.021416</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Controls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4004</th>\n",
       "      <td>0.466317</td>\n",
       "      <td>0.166003</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4005</th>\n",
       "      <td>0.278958</td>\n",
       "      <td>0.438798</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dementia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007</th>\n",
       "      <td>0.450932</td>\n",
       "      <td>0.160414</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4009</th>\n",
       "      <td>0.035249</td>\n",
       "      <td>0.735511</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dementia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       NMF_2_1   NMF_2_2  predicted  APOE4  DXCHANGE\n",
       "RID                                                 \n",
       "4003  0.497158  0.021416          0    1.0  Controls\n",
       "4004  0.466317  0.166003          1    0.0       MCI\n",
       "4005  0.278958  0.438798          2    1.0  Dementia\n",
       "4007  0.450932  0.160414          1    1.0       MCI\n",
       "4009  0.035249  0.735511          3    0.0  Dementia"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = M_chosen_item_24.dropna().index\n",
    "index2 = redued.dropna().index\n",
    "\n",
    "ind = set(index).intersection(set(index2) )\n",
    "\n",
    "redued['predicted'] = redued['predicted'].replace(['Controls','Low','Moderate','High'],[0,1,2,3])\n",
    "M_chosen_item_24 = M_chosen_item_24.loc[ind,:].sort_index()\n",
    "print(M_chosen_item_24.shape)\n",
    "redued = redued.loc[ind,:].sort_index()\n",
    "redued.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the 1 value\n",
      "for the 2 value\n",
      "for the 3 value\n",
      "for the 4 value\n",
      "for the 5 value\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(  M_chosen_item_24 ,  redued['predicted']   , test_size=0.2, random_state=i)\n",
    "    \n",
    "    x_train.to_csv('C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\sem4\\\\adni_lstm_predict_label_data\\\\progression_labels\\\\split_'+str(i)+'_'+visits+'\\\\train_data.csv' )\n",
    "    x_test.to_csv('C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\sem4\\\\adni_lstm_predict_label_data\\\\progression_labels\\\\split_'+str(i)+'_'+visits+'\\\\test_data.csv')\n",
    "\n",
    "    y_train.to_csv('C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\sem4\\\\adni_lstm_predict_label_data\\\\progression_labels\\\\split_'+str(i)+'_'+visits +'\\\\train_labels.csv')\n",
    "    y_test.to_csv('C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\sem4\\\\adni_lstm_predict_label_data\\\\progression_labels\\\\split_'+str(i)+'_'+visits +'\\\\test_labels.csv')\n",
    "    print('for the {} value'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split number 1\n",
      "Split number 2\n",
      "Split number 3\n",
      "Split number 4\n",
      "Split number 5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "address = 'C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\sem4\\\\adni_lstm_predict_label_data\\\\progression_labels\\\\split_'+str(i)+'_'+visits+'\\\\'\n",
    "os.chdir(address)\n",
    "\n",
    "# this will contain a dictionary. which will have 2  index named 'label' and 'data'\n",
    "def convert_data(data, data_label):\n",
    "    data_dict = {}\n",
    "    i = 0\n",
    "    for a in list( data.index ):\n",
    "        # creating empty dictionary to save data and label.\n",
    "        dict1 = {}\n",
    "        # Defining new data frame as per format\n",
    "        data_1 = pd.DataFrame(index = unique_cols, columns = ['bl','m06','m12'] )\n",
    "        for g in data.columns:\n",
    "            data_1.loc[g.split('___')[0], g.split('___')[1]] = data.loc[a,g]\n",
    "        # Checking if there are more than 3 null value\n",
    "        if max( data_1.isnull().sum(axis = 1) ) > 2:\n",
    "            print('error more null values {} '.format(data_1.isnull().sum(axis = 1)))\n",
    "            print(a)\n",
    "            print(data_1.head() )\n",
    "        # Filling nulls in row with average of other.\n",
    "        # in many cases data was only collected for say 'bl' and 'm06'\n",
    "        data_1 = data_1.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "        if data_1.isnull().sum(axis = 1).sum(axis = 0)  != 0:\n",
    "            print('null value in data')    \n",
    "        dict1['data'] = data_1\n",
    "        dict1['label'] = int( data_label.T[a] )\n",
    "        dict1['index'] = a\n",
    "        data_dict[i] = dict1\n",
    "        i += 1\n",
    "    return(data_dict)\n",
    "    \n",
    "# Reading raw data  \n",
    "for i in range(1,6):\n",
    "    print('Split number '+str(i))\n",
    "    address = 'C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\sem4\\\\adni_lstm_predict_label_data\\\\progression_labels\\\\split_'+str(i)+'_'+visits+'\\\\'\n",
    "    os.chdir(address)\n",
    "    data_test = pd.read_csv(address+'test_data.csv', index_col = 0)\n",
    "    data_label_test = pd.read_csv(address+'test_labels.csv', index_col = 0, header = None )\n",
    "    data = pd.read_csv(address+'train_data.csv', index_col = 0)\n",
    "    data_label = pd.read_csv(address+'train_labels.csv', index_col = 0, header = None )\n",
    "    list_columns = [ a.split('___')[0] for a in list( data.columns )     ]\n",
    "    unique_cols = list( set(list_columns) )\n",
    "\n",
    "\n",
    "    # converting raw data\n",
    "    data_dict = convert_data(data, data_label)\n",
    "    data_dict_test = convert_data(data_test, data_label_test)\n",
    "\n",
    "\n",
    "    # saving it as numpy object\n",
    "    with open('data_dict_test.pickle', 'wb') as handle:\n",
    "        pickle.dump(data_dict_test , handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('data_dict_test.pickle', 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "\n",
    "    with open('data_dict.pickle', 'wb') as handle:\n",
    "        pickle.dump(data_dict , handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('data_dict.pickle', 'rb') as handle:\n",
    "        a = pickle.load(handle)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################\n",
    "### Check if there are 3 null values in a row.\n",
    "#for a in data_dict:\n",
    "#    if max( data_dict[a]['data'].isnull().sum(axis = 1) ) > 2:\n",
    "#        print(a)\n",
    "#        print(max( data_dict[a]['data'].isnull().sum(axis = 1) ) > 2)\n",
    "#    else:\n",
    "#        pass   \n",
    "###############################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting split 1\n",
      "Test and Train Accuracy for 1: 80.0 and 93.75 %\n",
      "----END----\n",
      "\n",
      " Starting split 2\n",
      "Test and Train Accuracy for 2: 82.22222222222223 and 100.0 %\n",
      "----END----\n",
      "\n",
      " Starting split 3\n",
      "Test and Train Accuracy for 3: 81.11111111111111 and 93.75 %\n",
      "----END----\n",
      "\n",
      " Starting split 4\n",
      "Test and Train Accuracy for 4: 84.44444444444444 and 100.0 %\n",
      "----END----\n",
      "\n",
      " Starting split 5\n",
      "Test and Train Accuracy for 5: 81.11111111111111 and 100.0 %\n",
      "----END----\n",
      "Five fold cross validation average accuracy for prediction at visit m24  is 81.77777777777779 \n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jan 22 17:45:32 2019\n",
    "\n",
    "@author: Vipul Satone\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# NL = 0\n",
    "# MCI = 1\n",
    "# DEM = 2\n",
    "\n",
    "# Setting up random seed\n",
    "torch.manual_seed(12)\n",
    "torch.cuda.manual_seed(12)\n",
    "np.random.seed(12)\n",
    "random.seed(12)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "root = 'C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\sem4\\\\adni_lstm_predict_label_data\\\\progression_labels'\n",
    "\n",
    "os.chdir(root)\n",
    "\n",
    "class adniDataloader(torchUtils.Dataset):\n",
    "    def __init__(self, root, data_path):\n",
    "        self.root = root\n",
    "        with open(data_path, 'rb') as handle:\n",
    "            self.data = pickle.load(handle)\n",
    "        self.len = len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
    "#        print(index)\n",
    "        data_dict = self.data\n",
    "        image = np.array(data_dict[index]['data'])\n",
    "        target = np.array(data_dict[index]['label'])\n",
    "        return torch.tensor(image, dtype=torch.float, device=device), target\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Bidirectional recurrent neural network (many-to-one)\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        np.random.seed(1)\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True,dropout=dropout_per,bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)  # 2 for bidirection\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial states\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) # 2 for bidirection \n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "def train_test(data_path,data_path_test, split):\n",
    "    dataset = adniDataloader(root,data_path)\n",
    "    train_loader = torchUtils.DataLoader(dataset=dataset, \n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=True)\n",
    "    \n",
    "    model = BiRNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "    \n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            \n",
    "\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "#            if (i+1) % 1 == 0:\n",
    "#                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "#                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "    _, predicted_on_train = torch.max(outputs.data, 1)\n",
    "\n",
    "    train_acc = 100 * ( predicted_on_train.numpy() == labels.numpy() ).sum().item() / len(predicted_on_train.numpy())\n",
    "    \n",
    "    \n",
    "    \n",
    "    dataset = adniDataloader(root,data_path_test)\n",
    "    test_loader = torchUtils.DataLoader(dataset=dataset, \n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=True)\n",
    "    \n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    # Test the model\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.long()).sum().item()\n",
    "            list1.extend(predicted.numpy())\n",
    "            list2.extend(labels.numpy() )\n",
    "            test_acc = 100 * correct / total\n",
    "    \n",
    "        print('Test and Train Accuracy for {}: {} and {} %'.format( split, test_acc, train_acc) ) \n",
    "    \n",
    "    \n",
    "#    print('For {} split'.format(split) )\n",
    "#    solution = pd.DataFrame( np.column_stack((list1, list2 )) )\n",
    "#    solution = solution.replace([0,1,2],['NL','MCI','Dem'])\n",
    "#    \n",
    "#    solution['result'] = solution.apply(lambda x : str(x[1])+'->'+ str(x[0]) if x[0] != x[1]  else \"\", axis=1)\n",
    "#    print( solution['result'].value_counts() )\n",
    "    print('----END----')\n",
    "    return(test_acc)\n",
    "\n",
    "# Hyper-parameters\n",
    "sequence_length = 3\n",
    "input_size = 79\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "num_classes = 4\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "batch_size = 20\n",
    "dropout_per = 0.2\n",
    "\n",
    "total = 0\n",
    "for split in range(1,6):\n",
    "    print('\\n Starting split {}'.format(split))\n",
    "    data_path = 'C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\sem4\\\\adni_lstm_predict_label_data\\\\progression_labels\\\\split_'+str(i)+'_'+visits+'\\\\data_dict.pickle'\n",
    "    data_path_test = 'C:\\\\Users\\\\Vipul Satone\\\\health data\\\\ADNI\\\\sem4\\\\adni_lstm_predict_label_data\\\\progression_labels\\\\split_'+str(i)+'_'+visits+'\\\\data_dict_test.pickle'\n",
    "    acc = train_test(data_path,data_path_test, split)\n",
    "    total += acc\n",
    "print('Five fold cross validation average accuracy for prediction at visit {}  is {} '.format(visits, (total /split) ) )\n",
    "\n",
    "#with open('solution.pickle', 'wb') as handle:\n",
    "#    pickle.dump(solution , handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#\n",
    "#with open('solution.pickle', 'rb') as handle:\n",
    "#    a = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hda",
   "language": "python",
   "name": "hda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
